<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Spelling Error Correction with Soft-Masked BERT · Shayue'Log</title><meta name="description" content="Spelling Error Correction with Soft-Masked BERT - wangt"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/images/waterlemon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://shayue111.github.io/atom.xml" title="Shayue'Log"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Shayue'Log" type="application/atom+xml">
</head><body><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">POSTS</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/" target="_self" class="nav-list-link">CATEGORY</a></li><li class="nav-list-item"><a href="/tags/" target="_self" class="nav-list-link">TAG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    }
});</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});</script><script async type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div class="post"><article class="post-block"><h1 class="post-title">Spelling Error Correction with Soft-Masked BERT</h1><div class="post-info">Mar 28, 2021<a href="/tags/Paper-Reading/" class="tag-title"># Paper Reading</a><a href="/tags/Spelling-Error-Correction/" class="tag-title"># Spelling Error Correction</a></div><div class="post-content"><blockquote>
<p>Recently, I want to learn how to build a knowledge graph. In the period, I realize that <code>spelling error correction</code> is one of the most important links. So, I should find a paper, do some reading and take notes about it.</p>
</blockquote>
<a id="more"></a>
<h1>Challenges</h1>
<p>For the <code>Spelling Error Correction for Chinese Character</code>, there are mainly two challenges, displayed as follow:</p>
<ul>
<li>First, some mistaken is produces by written. E.g. :
<ul>
<li>Wrong: 埃及有金子塔。Egypt has golden towers.</li>
<li>Correct: 埃及有金字塔。Egypt has pyramids.</li>
<li>For this condition, we could correct it with the <code>world knowledge</code>.</li>
</ul>
</li>
<li>Another situation occurs when we need to correct it with <code>inference</code>. E.g:
<ul>
<li>Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks</li>
<li>Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks.</li>
</ul>
</li>
</ul>
<h1>Past Methods</h1>
<p>It exists mainly two categories methods, namely <code>traditional machine learning</code> and <code>deep learning</code>.</p>
<p>Some methods are displayed as below:</p>
<ol>
<li>a unified framework, which is consists of a pipeline of error detection, candidate generation and final candidate selection by traditional machine learning.</li>
<li>a <code>Seq2Seq</code> model with copy mechanism which transforms an input sentence into a new sentence with spelling errors corrected.</li>
<li>Bert based model. The dataset to fine-tuning the Bert can be generated by <code>a large confusion table</code>. In the inference period, Bert predict the most probability character for each position.</li>
</ol>
<p>In the above methods, we can get awesome accuracy with <code>Bert</code>. However, it seems that the result could be better with some change. In the origin bert, the model randomly select 15% words to mask, which results the model only learn the distribution of masked token and choose not to make any correction.</p>
<h1>Proposed method</h1>
<p>The proposed method in this paper is also Bert based. To address the aforementioned issue, the proposed model contains two network, which is detection network and another is correction network.</p>
<p>Below is the architecture of proposed model: <code>Soft-Masked BERT</code></p>
<p><img src="/images/soft-masked-bert-arch.jpg" alt="model architecture"><center style="font-size:14px;color:#C0C0C0">model architecture</center></p>
<p>As the figure illustrates, the model mainly contains two network. The correction network is <code>a Bi-GRU</code> network that predicts the probability whether the character is error for each position. And the correction network is the same as former Bert.</p>
<p><strong>Next, we’ll dive into the detail of training method of the <code>Soft-Masked BERT</code>.</strong></p>
<ol>
<li>Firstly, we should creates an embedding for each character in the input sentence, which is referred as the <code>input embedding</code>.</li>
<li>Next, we send the input embedding to <code>detection network</code> and get the output of probability of errors for the character in each position.</li>
<li>Now, we have the probability indicating whether it’s error in each position. We do weighted sum for the <code>input embedding</code> and <code>[MASK] embedding</code> by the error probabilities.
<ul>
<li><strong>To explain clearly, we can look the <code>architecture illustration</code> above. We take the <code>3rd position</code> for the example.</strong></li>
<li>With the detection network, we get the error probability of this position, denoted as $p_3$, which indicates how much the character in this position would be error. Therefore, the correct probability of the position is $1 - p_3$.</li>
<li>Meanwhile, we have the embedding of the <code>3rd position character</code> and <code>[MASK] token</code>. So we can do weighted sum for these two embedding, and the weight is just the $p_3$ and $1 - p_3$.</li>
</ul>
</li>
<li>Use the output of detection model as the input of the next correction model.</li>
<li>There is also a residual connection between the input embedding and the output of correction model. The combination of them will be sent to the softmax layer to predict the max probability of the character, which should be put on this position.</li>
</ol>
<h2 id="Training">Training</h2>
<p>This model is training end-to-end, although it contains two sub-model.</p>
<p>And the objective function for these two task are both <code>cross entropy</code>. To learning on a better way, this paper use a coefficient to combine these two loss, which is described as below:</p>
<p>\begin{equation}<br>
\boldsymbol \ell_{total} = \lambda \cdot \boldsymbol \ell_{c}+(1-\lambda) \cdot \boldsymbol \ell_{d}<br>
\end{equation}</p>
<p>where $\boldsymbol \ell_d$ is the objective for training of the detection network, and $\boldsymbol \ell_c$ is the objective for training of the correction network, which is also the final decision, and $\lambda$ is the coefficient.</p>
<h1>Reference</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.07421">https://arxiv.org/abs/2005.07421</a>, origin paper</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/posts/LSTM_Explained/" class="prev">← BACK</a><a href="/posts/BatchNormalization%20and%20LayerNormalization/" class="next">NEXT →</a></div><div class="copyright"><p>© 2020 - 2023 shayue.wt. Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">Apollo</a></p></div></footer></body><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></html>