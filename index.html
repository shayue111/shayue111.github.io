<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Shayue'Log</title><meta name="description" content="The personal blog of wangt."><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/images/waterlemon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://shayue111.github.io/atom.xml" title="Shayue'Log"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Shayue'Log" type="application/atom+xml">
</head><body><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/" target="_self" class="nav-list-link">CATEGORY</a></li><li class="nav-list-item"><a href="/tags/" target="_self" class="nav-list-link">TAG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    }
});</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});</script><script async type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Large-Language-Model/LoRA_Explained/" class="post-title-link">LoRA Explained</a></h2><div class="post-info">Apr 7, 2023<a href="/tags/Efficient-Framework/" class="tag-title"># Efficient Framework</a></div><div class="post-content"><blockquote>
<p>近些时间，大模型如雨后春笋般，突的一下，进入公众视野，诸如语言领域的ChatGPT，或是图像领域的Stable Diffusion。它们在各自领域上带给用户不俗的使用体验。在算法应用开发的角度，我们更关心能不能在特定的算法环境中使用上这些先进的大模型，而庞大的模型参数量为这个问题蒙上一些不确定性。本文要介绍的<code>LoRA</code>无疑是为大模型的训练提供了一种新的可能。</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Natural-Language-Processing/BERT_Explained/" class="post-title-link">BERT Explained</a></h2><div class="post-info">May 6, 2021<a href="/tags/Sequence-Model/" class="tag-title"># Sequence Model</a><a href="/tags/Transformers/" class="tag-title"># Transformers</a><a href="/tags/BERT-Family/" class="tag-title"># BERT Family</a></div><div class="post-content"><blockquote>
<p>BERT, which means $Bidirectional \ Encoder \ Representations \ from \ Transformers$, is one kind of SOAT models in natural language preprocessing over multiple tasks. In this arctile, I want to note my opnion of this model architecture and its training method.</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Natural-Language-Processing/LSTM_Explained/" class="post-title-link">LSTM Explained</a></h2><div class="post-info">Apr 1, 2021<a href="/tags/Sequence-Model/" class="tag-title"># Sequence Model</a></div><div class="post-content"><blockquote>
<p>在时序相关或者文本相关的任务中，人们常常会使用<code>RNN-Based</code>的序列模型，本文将主要介绍<code>LSTM</code>涉及到的一些知识点。主要参考几篇优秀的博文，链接将放在最后的<a href="#jump">Reference</a>中，感兴趣的建议直接阅读原文🚘。</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Natural-Language-Processing/paper_spelling_error_correction/" class="post-title-link">Spelling Error Correction with Soft-Masked BERT</a></h2><div class="post-info">Mar 28, 2021<a href="/tags/Paper-Reading/" class="tag-title"># Paper Reading</a><a href="/tags/Spelling-Error-Correction/" class="tag-title"># Spelling Error Correction</a></div><div class="post-content"><blockquote>
<p>Recently, I want to learn how to build a knowledge graph. In the period, I realize that <code>spelling error correction</code> is one of the most important links. So, I should find a paper, do some reading and take notes about it.</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/uncategorized/BatchNormalization%20and%20LayerNormalization/" class="post-title-link">BatchNormalization and LayerNormalization</a></h2><div class="post-info">Dec 21, 2020</div><div class="post-content"><blockquote>
<p>This post is to introduce Batch Normalization and Layer Normaliztion, which are of the $\textit{regularization}$ methods in $\textit{Deep Learning}$.</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Computer-Vision/Connectionist%20Temporal%20Classification/" class="post-title-link">Connectionist Temporal Classification</a></h2><div class="post-info">Dec 15, 2020<a href="/tags/Text-Recognition/" class="tag-title"># Text Recognition</a><a href="/tags/Loss/" class="tag-title"># Loss</a></div><div class="post-content"><blockquote>
<p>由于需要进行图片文本识别相关的工作，最近接触到了CTC Loss，打算将这几天的学习进行归纳总结。如果有接触过<code>HMM</code>的相关内容，看到<code>CTC</code>可能会觉得很亲切。</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Computer-Vision/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84mAP%E4%B8%8EIOU/" class="post-title-link">目标检测中的mAP与IOU</a></h2><div class="post-info">Dec 9, 2020<a href="/tags/Object-Detection/" class="tag-title"># Object Detection</a><a href="/tags/Metric/" class="tag-title"># Metric</a></div><div class="post-content"><blockquote>
<p>本文将介绍目标检测任务（Object Detection）中的一般评价指标(Metric) - mAP（mean average Precison）以及IOU（Intersection over Union）。</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Computer-Vision/%E7%90%86%E8%A7%A3R-CNN/" class="post-title-link">理解R-CNN</a></h2><div class="post-info">Dec 3, 2020<a href="/tags/Object-Detection/" class="tag-title"># Object Detection</a><a href="/tags/R-CNN-Family/" class="tag-title"># R-CNN Family</a></div><div class="post-content"><blockquote>
<p>最近需要使用目标检测对表格内容提取，故决定将该领域的学习以博客的形式记录下来。这是系列文章的第一部分，主要介绍R-CNN的模型结构，特别是弄懂论文中如何训练R-CNN。本文参考了几篇优秀的文章，如果感兴趣尽可以跳到最后阅读原文。</p>
</blockquote></div></article></li></ul></main><footer><div class="paginator"></div><div class="copyright"><p>© 2020 - 2023 by shayue.wt. All Rights Reserved.</p></div></footer></body><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></html>