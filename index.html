<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Shayue'Log</title><meta name="description" content="The personal blog of wangt."><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/images/waterlemon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://shayue111.github.io/atom.xml" title="Shayue'Log"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Shayue'Log" type="application/atom+xml">
</head><body><header><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/" target="_self" class="nav-list-link">CATEGORY</a></li><li class="nav-list-item"><a href="/tags/" target="_self" class="nav-list-link">TAG</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    }
});</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});</script><script async type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Large-Language-Model/LoRA_Explained/" class="post-title-link">LoRA Explained</a></h2><div class="post-info">Apr 7, 2023<a href="/tags/Efficient-Framework/" class="tag-title"># Efficient Framework</a></div><div class="post-content"><blockquote>
<p>è¿‘äº›æ—¶é—´ï¼Œå¤§æ¨¡å‹å¦‚é›¨åæ˜¥ç¬‹èˆ¬ï¼Œçªçš„ä¸€ä¸‹ï¼Œè¿›å…¥å…¬ä¼—è§†é‡ï¼Œè¯¸å¦‚è¯­è¨€é¢†åŸŸçš„ChatGPTï¼Œæˆ–æ˜¯å›¾åƒé¢†åŸŸçš„Stable Diffusionã€‚å®ƒä»¬åœ¨å„è‡ªé¢†åŸŸä¸Šå¸¦ç»™ç”¨æˆ·ä¸ä¿—çš„ä½¿ç”¨ä½“éªŒã€‚åœ¨ç®—æ³•åº”ç”¨å¼€å‘çš„è§’åº¦ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒèƒ½ä¸èƒ½åœ¨ç‰¹å®šçš„ç®—æ³•ç¯å¢ƒä¸­ä½¿ç”¨ä¸Šè¿™äº›å…ˆè¿›çš„å¤§æ¨¡å‹ï¼Œè€Œåºå¤§çš„æ¨¡å‹å‚æ•°é‡ä¸ºè¿™ä¸ªé—®é¢˜è’™ä¸Šä¸€äº›ä¸ç¡®å®šæ€§ã€‚æœ¬æ–‡è¦ä»‹ç»çš„<code>LoRA</code>æ— ç–‘æ˜¯ä¸ºå¤§æ¨¡å‹çš„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„å¯èƒ½ã€‚</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Natural-Language-Processing/BERT_Explained/" class="post-title-link">BERT Explained</a></h2><div class="post-info">May 6, 2021<a href="/tags/Sequence-Model/" class="tag-title"># Sequence Model</a><a href="/tags/Transformers/" class="tag-title"># Transformers</a><a href="/tags/BERT-Family/" class="tag-title"># BERT Family</a></div><div class="post-content"><blockquote>
<p>BERT, which means $Bidirectional \ Encoder \ Representations \ from \ Transformers$, is one kind of SOAT models in natural language preprocessing over multiple tasks. In this arctile, I want to note my opnion of this model architecture and its training method.</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Natural-Language-Processing/LSTM_Explained/" class="post-title-link">LSTM Explained</a></h2><div class="post-info">Apr 1, 2021<a href="/tags/Sequence-Model/" class="tag-title"># Sequence Model</a></div><div class="post-content"><blockquote>
<p>åœ¨æ—¶åºç›¸å…³æˆ–è€…æ–‡æœ¬ç›¸å…³çš„ä»»åŠ¡ä¸­ï¼Œäººä»¬å¸¸å¸¸ä¼šä½¿ç”¨<code>RNN-Based</code>çš„åºåˆ—æ¨¡å‹ï¼Œæœ¬æ–‡å°†ä¸»è¦ä»‹ç»<code>LSTM</code>æ¶‰åŠåˆ°çš„ä¸€äº›çŸ¥è¯†ç‚¹ã€‚ä¸»è¦å‚è€ƒå‡ ç¯‡ä¼˜ç§€çš„åšæ–‡ï¼Œé“¾æ¥å°†æ”¾åœ¨æœ€åçš„<a href="#jump">Reference</a>ä¸­ï¼Œæ„Ÿå…´è¶£çš„å»ºè®®ç›´æ¥é˜…è¯»åŸæ–‡ğŸš˜ã€‚</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Natural-Language-Processing/paper_spelling_error_correction/" class="post-title-link">Spelling Error Correction with Soft-Masked BERT</a></h2><div class="post-info">Mar 28, 2021<a href="/tags/Paper-Reading/" class="tag-title"># Paper Reading</a><a href="/tags/Spelling-Error-Correction/" class="tag-title"># Spelling Error Correction</a></div><div class="post-content"><blockquote>
<p>Recently, I want to learn how to build a knowledge graph. In the period, I realize that <code>spelling error correction</code> is one of the most important links. So, I should find a paper, do some reading and take notes about it.</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/uncategorized/BatchNormalization%20and%20LayerNormalization/" class="post-title-link">BatchNormalization and LayerNormalization</a></h2><div class="post-info">Dec 21, 2020</div><div class="post-content"><blockquote>
<p>This post is to introduce Batch Normalization and Layer Normaliztion, which are of the $\textit{regularization}$ methods in $\textit{Deep Learning}$.</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Computer-Vision/Connectionist%20Temporal%20Classification/" class="post-title-link">Connectionist Temporal Classification</a></h2><div class="post-info">Dec 15, 2020<a href="/tags/Text-Recognition/" class="tag-title"># Text Recognition</a><a href="/tags/Loss/" class="tag-title"># Loss</a></div><div class="post-content"><blockquote>
<p>ç”±äºéœ€è¦è¿›è¡Œå›¾ç‰‡æ–‡æœ¬è¯†åˆ«ç›¸å…³çš„å·¥ä½œï¼Œæœ€è¿‘æ¥è§¦åˆ°äº†CTC Lossï¼Œæ‰“ç®—å°†è¿™å‡ å¤©çš„å­¦ä¹ è¿›è¡Œå½’çº³æ€»ç»“ã€‚å¦‚æœæœ‰æ¥è§¦è¿‡<code>HMM</code>çš„ç›¸å…³å†…å®¹ï¼Œçœ‹åˆ°<code>CTC</code>å¯èƒ½ä¼šè§‰å¾—å¾ˆäº²åˆ‡ã€‚</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Computer-Vision/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84mAP%E4%B8%8EIOU/" class="post-title-link">ç›®æ ‡æ£€æµ‹ä¸­çš„mAPä¸IOU</a></h2><div class="post-info">Dec 9, 2020<a href="/tags/Object-Detection/" class="tag-title"># Object Detection</a><a href="/tags/Metric/" class="tag-title"># Metric</a></div><div class="post-content"><blockquote>
<p>æœ¬æ–‡å°†ä»‹ç»ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼ˆObject Detectionï¼‰ä¸­çš„ä¸€èˆ¬è¯„ä»·æŒ‡æ ‡(Metric) - mAPï¼ˆmean average Precisonï¼‰ä»¥åŠIOUï¼ˆIntersection over Unionï¼‰ã€‚</p>
</blockquote></div></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/Computer-Vision/%E7%90%86%E8%A7%A3R-CNN/" class="post-title-link">ç†è§£R-CNN</a></h2><div class="post-info">Dec 3, 2020<a href="/tags/Object-Detection/" class="tag-title"># Object Detection</a><a href="/tags/R-CNN-Family/" class="tag-title"># R-CNN Family</a></div><div class="post-content"><blockquote>
<p>æœ€è¿‘éœ€è¦ä½¿ç”¨ç›®æ ‡æ£€æµ‹å¯¹è¡¨æ ¼å†…å®¹æå–ï¼Œæ•…å†³å®šå°†è¯¥é¢†åŸŸçš„å­¦ä¹ ä»¥åšå®¢çš„å½¢å¼è®°å½•ä¸‹æ¥ã€‚è¿™æ˜¯ç³»åˆ—æ–‡ç« çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œä¸»è¦ä»‹ç»R-CNNçš„æ¨¡å‹ç»“æ„ï¼Œç‰¹åˆ«æ˜¯å¼„æ‡‚è®ºæ–‡ä¸­å¦‚ä½•è®­ç»ƒR-CNNã€‚æœ¬æ–‡å‚è€ƒäº†å‡ ç¯‡ä¼˜ç§€çš„æ–‡ç« ï¼Œå¦‚æœæ„Ÿå…´è¶£å°½å¯ä»¥è·³åˆ°æœ€åé˜…è¯»åŸæ–‡ã€‚</p>
</blockquote></div></article></li></ul></main><footer><div class="paginator"></div><div class="copyright"><p>Â© 2020 - 2023 by shayue.wt. All Rights Reserved.</p></div></footer></body><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></html>