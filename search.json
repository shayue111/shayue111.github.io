[{"title":"ç†è§£R-CNN","url":"/posts/%E7%90%86%E8%A7%A3R-CNN/","content":"<blockquote>\n<p>æœ€è¿‘éœ€è¦ä½¿ç”¨ç›®æ ‡æ£€æµ‹å¯¹è¡¨æ ¼å†…å®¹æå–ï¼Œæ•…å†³å®šå°†è¯¥é¢†åŸŸçš„å­¦ä¹ ä»¥åšå®¢çš„å½¢å¼è®°å½•ä¸‹æ¥ã€‚è¿™æ˜¯ç³»åˆ—æ–‡ç« çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œä¸»è¦ä»‹ç»R-CNNçš„æ¨¡å‹ç»“æ„ï¼Œç‰¹åˆ«æ˜¯å¼„æ‡‚è®ºæ–‡ä¸­å¦‚ä½•è®­ç»ƒR-CNNã€‚æœ¬æ–‡å‚è€ƒäº†å‡ ç¯‡ä¼˜ç§€çš„æ–‡ç« ï¼Œå¦‚æœæ„Ÿå…´è¶£å°½å¯ä»¥è·³åˆ°æœ€åé˜…è¯»åŸæ–‡ã€‚</p>\n</blockquote>\n<a id=\"more\"></a>\n<h2 id=\"1-ç®€ä»‹\">1. ç®€ä»‹</h2>\n<p>æœ¬æ–‡å°†è¦ä»‹ç»çš„æ˜¯2014å¹´æ¥è‡ªUC Berkeleyçš„ä¸€ç¯‡è®ºæ–‡<a href=\"https://arxiv.org/pdf/1311.2524.pdf\">ã€ŠRich feature hierarchies for accurate object detection and semantic segmentationã€‹</a>ã€‚è®ºæ–‡æ‘’å¼ƒä¼ ç»Ÿæ‰‹å·¥æå–ç‰¹å¾çš„åšæ³•ï¼Œè½¬è€Œä½¿ç”¨CNNæ¶æ„æå–å›¾åƒç‰¹å¾ã€‚è™½ç„¶ç°åœ¨çœ‹æ¥ï¼ŒR-CNNçš„æ¶æ„æœ‰ä¸€å®šçš„ä¸è¶³ï¼Œä½†æ˜¯å¼•å…¥CNNçš„åšæ³•ä½¿å¾—ç›®æ ‡æ£€æµ‹é¢†åŸŸæœ‰äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>\n<p>ä¸‹é¢æˆ‘å°†å°½å¯èƒ½ç®€ç•¥åœ°ä»‹ç»R-CNNçš„æ€æƒ³ä»¥åŠå…¶è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨ä»‹ç»å¼€å§‹å‰ï¼Œæˆ‘ä¼šå‡è®¾æ‚¨å¯¹DNNã€CNNä»¥åŠåˆ†ç±»ä»»åŠ¡å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡å·²ç»æœ‰ä¸€å®šçš„äº†è§£ã€‚</p>\n<h2 id=\"2-æ¨¡å‹ä»‹ç»\">2. æ¨¡å‹ä»‹ç»</h2>\n<p>R-CNNçš„è¿ä½œæ–¹å¼å¦‚ä¸‹ï¼š</p>\n<ol>\n<li>è¾“å…¥ä¸€å¼ å®Œæ•´çš„å›¾ç‰‡ï¼›</li>\n<li>ä»ä¸­æå–å¤§çº¦2000ä¸ªregion proposals;</li>\n<li>å°†è¿™äº›æå–å‡ºæ¥çš„region proposalsé€å…¥CNNæå–ç‰¹å¾ï¼›</li>\n<li>å°†CNNè½¬åŒ–å¾—åˆ°çš„ç‰¹å¾é€å…¥çº¿æ€§åˆ†ç±»å™¨SVMä¸­è¿›è¡Œåˆ†ç±»ã€‚</li>\n</ol>\n<p>ä¸‹å›¾æ˜¯è®ºæ–‡ä¸­ç»™å‡ºçš„ç¤ºæ„å›¾ï¼š</p>\n<p><img src=\"/images/R-CNN/rcnn.png\" alt=\"R-CNN Overview\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">R-CNN Overview, from https://arxiv.org/pdf/1311.2524.pdf</center>\n<p>R-CNNæœ€å€¼å¾—ç§°èµçš„åœ°æ–¹æ— ç–‘æ˜¯é‡‡ç”¨region proposalsåŠ ä¸ŠCNNè¿›ä¸€æ­¥æå–ç‰¹å¾ã€‚</p>\n<h4 id=\"2-1-æå–Region-Proposals\">2.1. æå–Region Proposals</h4>\n<p>æˆ‘ä»¬éœ€è¦ä¸€äº›ç®—æ³•å»æå–Regionã€‚ç®—æ³•çš„è¾“å…¥æ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œè¾“å‡ºæ˜¯è‹¥å¹²ä¸ªå¯èƒ½åŒ…å«ç›®æ ‡ç‰©ä½“çš„åŒºåŸŸï¼Œæ¯”å¦‚å¯ä»¥ä½¿&lt;$c_x, c_y, H, W$&gt;è¡¨ç¤ºä¸€ä¸ªæå–å‡ºçš„åŒºåŸŸï¼Œå‰ä¸¤ä¸ªå˜é‡æ˜¯Anchor Boxçš„ä½ç½®åæ ‡ï¼Œåä¸¤ä¸ªå˜é‡æ˜¯å…¶é«˜åº¦ä¸å®½åº¦ã€‚</p>\n<p>åˆæ­¥å¾—åˆ°çš„å€™é€‰åŒºåŸŸéå¸¸çš„<code>noisy</code>ï¼Œå®ƒä»¬ä¸­å¯èƒ½ç¡®å®åŒ…å«éœ€è¦è¢«æ£€æµ‹çš„<code>object</code>ï¼Œå¯èƒ½ä¹Ÿæ²¡æœ‰ã€‚åŒæ—¶ï¼Œè¾“å‡ºçš„&lt;$c_x, c_y, H, W$&gt;ä¹Ÿä¸ä¸€å®šå‡†ç¡®ï¼Œå¦‚ä¸‹å›¾æ˜¯ä¸€å¼ å›¾ç‰‡ä¸­æå–å‡ºçš„è‹¥å¹²Region Proposalsï¼Œå…¶ä¸­ç»¿è‰²æ¡†å¯ä»¥è¢«è®¤ä¸ºæ˜¯æˆ‘ä»¬æœŸå¾…è¾“å‡ºçš„Anchor Boxï¼Œè€Œè“è‰²æ¡†åˆ™æ˜¯æœ‰ç¼ºé™·çš„ã€‚</p>\n<p><img src=\"https://www.learnopencv.com/wp-content/uploads/2017/10/object-recognition-false-positives-true-positives.jpg\" alt=\"Region Proposalsç¤ºæ„\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">Region Proposals Example, from https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/</center>\n<p>Region proposal algorithmsç®—æ³•åŸºäºé¢œè‰²ã€çº¹ç†ç­‰å¯¹ä¸€å¼ å›¾ç‰‡è¿›è¡Œåˆ‡å‰²ï¼Œä½¿å¾—ä¸€äº›è¿‘ä¼¼çš„åŒºåŸŸè¿æ¥åœ¨ä¸€èµ·äº§ç”ŸRegionã€‚Selective Searchæ˜¯å…¶ä¸­çš„ä¸€ç§ï¼Œä¹Ÿæ˜¯è¢«åº”ç”¨åœ¨è®ºæ–‡ä¸­çš„ç®—æ³•ã€‚æ›´å…·ä½“çš„å†…å®¹ï¼Œæœ¬æ–‡ä¸å†é˜è¿°ï¼Œæœ‰å…´è¶£çš„è¯»è€…å¯ä»¥è®¿é—®<a href=\"https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/\">æ–‡ç« </a>ï¼Œæœ‰ç€éå¸¸è¯¦å°½çš„ä»‹ç»ï¼Œå¹¶ä¸”ç»™å‡ºäº†ç›¸åº”çš„å®ç°ä»£ç ã€‚</p>\n<h4 id=\"2-2-CNN\">2.2. CNN</h4>\n<p>åœ¨å¾—åˆ°Region proposalsåï¼Œå°†æ¯ä¸€ä¸ªåŒºåŸŸé€åˆ°CNNæ¶æ„ä¸­ï¼Œå¾—åˆ°è¯¥åŒºåŸŸçš„ç‰¹å¾å‘é‡ï¼Œåœ¨è®ºæ–‡ä¸­ä½¿ç”¨çš„CNNæ˜¯<code>AlexNet</code>ã€‚</p>\n<p><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png\" alt=\"AlexNet Architecture\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">AlexNet Architecture, from https://www.learnopencv.com/understanding-alexnet/</center>\n<p><strong>ç¬¬ä¸€ä¸ªé—®é¢˜æ¥äº†</strong>ï¼Œè¿™éƒ¨åˆ†CNNéœ€è¦æ€ä¹ˆæ ·è¢«è®­ç»ƒï¼Ÿå› ä¸ºæ¨¡å‹åæ¥çš„æ˜¯è‹¥å¹²ä¸ªSVMåˆ†ç±»å™¨ï¼Œæ˜¾ç„¶ä¸¤è€…æ— æ³•ä¸€èµ·è®­ç»ƒã€‚</p>\n<p>è®ºæ–‡ä¸­çš„åšæ³•æ˜¯å¯¹CNNéƒ¨åˆ†è¿›è¡Œ<strong>é¢„è®­ç»ƒ</strong>ï¼Œå…·ä½“è®­ç»ƒè¿™éƒ¨åˆ†çš„å‚æ•°å°†åœ¨ä¹‹åä»‹ç»ã€‚åœ¨AlexNetçš„æ¨¡å‹ç¡®å®šä¹‹åï¼Œå°†å…¶æœ€åä¸€å±‚å»æ‰ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œé€šè¿‡CNNåï¼Œä¹‹å‰çš„Region proposalsè¢«è½¬åŒ–è‡³4096ç»´çš„å‘é‡ã€‚</p>\n<p><strong>å¦å¤–ä¸€ä¸ªé—®é¢˜æ˜¯</strong>ï¼ŒAlexNetçš„è¾“å…¥æ˜¯å›ºå®šçš„ï¼Œå³(227, 227, 3)ï¼Œè€Œä¹‹å‰æå–å‡ºçš„Region proposalså½¢çŠ¶ä¸ä¸€ï¼Œå¦‚ä½•è¾“å…¥AlexNetï¼Ÿè¿™å°±éœ€è¦å¯¹æ¯ä¸ªRegionè¿›è¡Œresizeçš„æ“ä½œã€‚</p>\n<h4 id=\"2-3-SVM\">2.3. SVM</h4>\n<p>åœ¨è·å¾—ç‰¹å¾å‘é‡åï¼Œä¸‹ä¸€æ­¥æ˜¯å°†å…¶é€å…¥è‹¥å¹²ä¸ªSVMåˆ†ç±»å™¨ä¸­ã€‚æ¯”å¦‚ï¼Œç›®æ ‡æ£€æµ‹çš„å¯¹è±¡æœ‰10ç±»ï¼Œé‚£ä¹ˆæ€»å…±è¦é€å…¥10ä¸ªSVMåˆ†ç±»å™¨ã€‚</p>\n<h4 id=\"2-4-ä¿®æ­£Anchor-Box\">2.4. ä¿®æ­£Anchor Box</h4>\n<p>åˆ°ä¸Šä¸€æ­¥ä¸ºæ­¢ï¼ŒR-CNNçš„é¢„æµ‹æµç¨‹å·²ç»èµ°å®Œäº†ä¸€å¤§åŠã€‚æœ€åè¿˜æœ‰ä¸€ä¸ªç¯èŠ‚å°±æ˜¯ç¡®å®šç›®æ ‡æ‰€åœ¨çš„ä½ç½®ï¼Œå³ç¡®å®š&lt;$c_x, c_y, H, W$&gt;ã€‚</p>\n<p>ä¹‹å‰åœ¨æå–å€™é€‰åŒºåŸŸçš„æ—¶å€™ï¼Œå·²ç»æœ‰ä¸€ä¸ªåˆæ­¥çš„&lt;$c_x, c_y, H, W$&gt;$_{prior}$ï¼Œæ¥ä¸‹æ¥ä¾¿æ˜¯å¯¹å·²ç»ç¡®å®æ˜¯ç›®æ ‡çš„regionè¿›è¡Œä¿®æ­£ã€‚å…³äºä¿®æ­£çš„ç®—æ³•ï¼Œæœ¬è´¨ä¸Šåˆ©ç”¨ä¸€ä¸ªå›å½’å™¨è¿›è¡Œæ‹Ÿåˆã€‚</p>\n<p>è¿™æ ·ï¼Œæˆ‘ä»¬ä¾¿èµ°å®Œäº†R-CNNçš„å…¨éƒ¨æµç¨‹ã€‚</p>\n<h2 id=\"3-R-CNNçš„è®­ç»ƒ\">3. R-CNNçš„è®­ç»ƒ</h2>\n<p>åœ¨è¿™ä¸€å°èŠ‚ï¼Œæˆ‘å°†ä¸»è¦ä»‹ç»å¦‚R-CNNçš„è®­ç»ƒè¿‡ç¨‹ã€‚R-CNNçš„ä¸€ä¸ªé‡å¤§ç¼ºé™·å°±åœ¨äºå®ƒä¸æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œæ— æ³•æŠŠæ‰€æœ‰çš„ç»„ä»¶åˆå¹¶åˆ°ä¸€ä¸ªpipelineä¸€èµ·è®­ç»ƒå‚æ•°ã€‚è®ºæ–‡ä¸­çš„è§£å†³åŠæ³•æ˜¯åˆ†å¼€è®­ç»ƒï¼Œä¸‹é¢å…·ä½“å±•å¼€ä»‹ç»ã€‚</p>\n<p><strong>CNNéƒ¨åˆ†</strong>ï¼Œè®ºæ–‡ä½¿ç”¨é¢„è®­ç»ƒæ–¹å¼ç¡®å®šç½‘ç»œå‚æ•°ã€‚é¦–å…ˆï¼Œä½¿ç”¨é€‰å®šçš„ç½‘ç»œæ¶æ„ï¼ˆæœ¬ä¾‹æ˜¯AlexNetï¼‰åœ¨æ•°æ®é›†<a href=\"http://image-net.org/challenges/LSVRC/2012/\">ILSVRC 2012 classification dataset</a>è¿›è¡Œè®­ç»ƒã€‚è¯¥æ•°æ®é›†ä¸­åŒ…å«1000ä¸ªä¸åŒç±»åˆ«çš„å›¾ç‰‡ï¼Œå½¼æ—¶çš„ä»»åŠ¡æ˜¯å¤šåˆ†ç±»ã€‚</p>\n<p>åœ¨ä¸Šä¸ªä»»åŠ¡è®­ç»ƒç»“æŸåï¼Œå°†è®­ç»ƒå¥½çš„ç½‘ç»œç”¨äºå¦ä¸€ä¸ªä»»åŠ¡çš„è®­ç»ƒï¼Œå³ä½¿ç”¨<a href=\"http://host.robots.ox.ac.uk/pascal/VOC/\">PASCAL VOC dataset</a>æˆ–<a href=\"http://image-net.org/challenges/LSVRC/2013/\">ILSVRC 2013 detection dataset</a>è¿›è¡Œfine-tuningã€‚ä¸Šä¸€ä¸ªä»»åŠ¡æ˜¯æ¶‰åŠ1000ä¸ªç±»åˆ«çš„å¤šåˆ†ç±»ï¼Œåœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œå°†ä¹‹å‰CNNçš„æœ€åä¸€å±‚å»æ‰ï¼ŒåŠ ä¸Šæ–°çš„ä¸€å±‚ï¼Œå¯¹<code>N+1</code>ç§ç±»åˆ«åˆ†ç±»ï¼Œæ­¤å¤„çš„<code>N</code>æ˜¯æ•°æ®é›†ä¸­çš„ç±»åˆ«ä¸ªæ•°ã€‚</p>\n<p>åŒæ—¶ï¼Œç½‘ç»œçš„è¾“å…¥ä¹Ÿæœ‰æ‰€å˜åŒ–ã€‚åœ¨pre-trainingé˜¶æ®µçš„è¾“å…¥æ˜¯æ•´å¼ å›¾ç‰‡ï¼Œåœ¨fine-tuningé˜¶æ®µï¼Œè¾“å…¥åˆ™æ˜¯<code>warped region proposals</code>ï¼Œå³ç­›é€‰å‡ºçš„2000ä¸ªå·¦å³çš„å€™é€‰åŒºåŸŸã€‚è¿™å°±éœ€è¦é‡æ–°æ„å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†ï¼Œå°†å€™é€‰åŒºåŸŸä¸ground-truth bounding boxè®¡ç®—å¾—åˆ°çš„<code>IOU &gt; 0.5</code>çš„è§†ä¸ºæ­£ä¾‹ï¼Œåä¹‹è§†ä¸ºè´Ÿä¾‹ï¼ˆå³é¢„æµ‹å‡ºæ¥ä¸ºèƒŒæ™¯ï¼‰ã€‚è¿˜æœ‰ä¸€äº›è®­ç»ƒçš„å‚æ•°è®¾ç½®ï¼Œè¿™é‡Œä¸è¿‡å¤šé˜è¿°ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥æŸ¥çœ‹è®ºæ–‡ã€‚</p>\n<p>æœ€åï¼ŒSVMéƒ¨åˆ†è€ƒè™‘ä½¿ç”¨ç®€å•çš„äºŒåˆ†ç±»ã€‚æ¯”å¦‚æ£€æµ‹region proposalæ˜¯å¦å«æœ‰è½¦è¾†ï¼Œæœ‰è½¦çš„è¢«è§†ä¸ºæ­£ä¾‹ï¼Œæ— è½¦çš„è¢«è§†ä¸ºè´Ÿä¾‹ã€‚é—®é¢˜ä¸»è¦åœ¨äºï¼Œå¦‚æœåˆ¤å®šregion proposalæ˜¯å¦åŒ…å«æ­£ä¾‹ï¼Œè®ºæ–‡ä»¥<code>IOU=0.3</code>ä¸ºç•Œé™ï¼Œè¿™æ˜¯é€šè¿‡éªŒè¯é›†å¾—å‡ºçš„å€¼ã€‚</p>\n<p>åˆ†ç±»éƒ¨åˆ†çš„è®­ç»ƒå¤§è‡´å¦‚ä¸Šæ‰€è¿°ï¼Œè¿˜æœ‰ä¸€ä¸ªå›å½’å™¨éœ€è¦è®­ç»ƒï¼Œç”¨äºå¯¹Anchor Boxçš„ä¿®æ­£ï¼Œè®ºæ–‡ä½¿ç”¨çš„ä¿®æ­£æ¨¡å‹æ˜¯Ridge Regressionã€‚è®­ç»ƒå¾—åˆ°å‚æ•°åï¼Œå°†&lt;$c_x, c_y, H, W$&gt;$_{prior}$ä»¥ä¸€å®šæ–¹å¼è½¬åŒ–åˆ°&lt;$c_x, c_y, H, W$&gt;ã€‚</p>\n<h2 id=\"4-æ€»ç»“\">4. æ€»ç»“</h2>\n<p>æœ¬æ–‡ä¸»è¦å¯¹R-CNNè¿›è¡Œäº†ç®€å•çš„ä»‹ç»ï¼ŒåŒ…æ‹¬å…¶é¢„æµ‹æµç¨‹ä»¥åŠè®­ç»ƒæµç¨‹ã€‚å¯¹äºå…¶ä¸­ä¸€äº›ç»†èŠ‚å¹¶æ²¡æœ‰è¿‡å¤šçš„æ·±å…¥ï¼Œå› ä¸ºæœ¬äººè®¤ä¸ºR-CNNåœ¨2020å¹´çœ‹æ¥ç¡®å®æœ‰äº›â€œè€â€ï¼Œä¸»è¦çš„ç¼ºé™·é›†ä¸­åœ¨è®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦æ…¢ï¼Œæ— æ³•ç«¯åˆ°ç«¯è®­ç»ƒç­‰ã€‚é¢„æµ‹æ—¶é¦–å…ˆè¦æå–å¤šä¸ªregion proposalsï¼Œæ¥ä¸‹æ¥å¯¹äºæ¯ä¸€ä¸ªregion proposalè½¬åŒ–å¾—åˆ°çš„ç‰¹å¾å‘é‡ï¼Œåœ¨åˆ†ç±»é˜¶æ®µè¿˜éœ€è¦é€å…¥è‹¥å¹²ä¸ªäºŒåˆ†ç±»SVMï¼Œæ•´ä½“çš„æ•ˆç‡å¤§æ‰“æŠ˜æ‰£ã€‚</p>\n<p>ä½†æ˜¯å…¶ä¸­çš„ä¸€äº›è®­ç»ƒæ€æƒ³æ˜¯å¾ˆå€¼å¾—å€Ÿé‰´çš„ï¼Œå…ˆä½¿ç”¨pre-traingï¼Œå†ä½¿ç”¨fine-tuingå·²ç»æ˜¯å½“å‰ç½‘ç»œè®­ç»ƒçš„åŸºæœ¬æ“ä½œã€‚åŒæ—¶ï¼Œæ•´ç¯‡è®ºæ–‡é€»è¾‘æ¸…æ™°ï¼Œç®€å•æ˜“æ‡‚ï¼Œæ˜¯å¾ˆå¥½çš„æ–‡ç« ã€‚</p>\n<h1>Reference</h1>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1311.2524.pdf\">https://arxiv.org/pdf/1311.2524.pdf</a> ï¼ŒR-CNNè®ºæ–‡</li>\n<li><a href=\"https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/\">https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/</a> ï¼Œä»‹ç»Selective Search</li>\n<li><a href=\"https://medium.com/@selfouly/r-cnn-3a9beddfd55a\">https://medium.com/@selfouly/r-cnn-3a9beddfd55a</a> ï¼Œä»‹ç»R-CNN</li>\n<li><a href=\"https://www.mihaileric.com/posts/object-detection-with-rcnn/\">https://www.mihaileric.com/posts/object-detection-with-rcnn/</a> ï¼Œä»‹ç»R-CNN</li>\n<li><a href=\"https://www.jianshu.com/p/9bcbf6d98238\">https://www.jianshu.com/p/9bcbf6d98238</a> ï¼Œä»‹ç»R-CNNï¼Œä¸­æ–‡</li>\n</ul>\n","categories":["Computer Vision"],"tags":["Object Detection","R-CNN Family"]},{"title":"ç›®æ ‡æ£€æµ‹ä¸­çš„mAPä¸IOU","url":"/posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84mAP%E4%B8%8EIOU/","content":"<blockquote>\n<p>æœ¬æ–‡å°†ä»‹ç»ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼ˆObject Detectionï¼‰ä¸­çš„ä¸€èˆ¬è¯„ä»·æŒ‡æ ‡(Metric) - mAPï¼ˆmean average Precisonï¼‰ä»¥åŠIOUï¼ˆIntersection over Unionï¼‰ã€‚</p>\n</blockquote>\n<a id=\"more\"></a>\n<h2 id=\"Precision-Recall-Curve\">Precision Recall Curve</h2>\n<p>åœ¨å¤šåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹åœ¨å¯¹æŸä¸ªæ ·æœ¬é¢„æµ‹åä¼šå¾—åˆ°ä¸€ä¸ªæ¦‚ç‡ï¼Œå³å±äº$c_i$ç±»çš„æ¦‚ç‡ä¸º$p_i$ï¼Œé€šå¸¸ä¼šç”¨PRæ›²çº¿(Precision Recall Curve)åˆ¤å®šæ¨¡å‹çš„æ€§èƒ½ã€‚</p>\n<p>å‡è®¾ç°åœ¨éœ€è¦ç”»å‡ºé’ˆå¯¹ç±»åˆ«$c_i$çš„PRæ›²çº¿ï¼Œå…·ä½“åšæ³•æ˜¯ï¼š</p>\n<ol>\n<li>è®¾ç½®å¤šä¸ªé˜ˆå€¼(threshold)ï¼Œå¦‚æœç±»åˆ«$c_i$å¯¹åº”çš„æ¦‚ç‡ä¸º$p_i$ï¼Œå¦‚æœå¤§äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯True Positiveï¼Œåä¹‹å±äºFalse Positiveï¼›</li>\n<li>step1ä¸­è®¾ç½®å‡ºçš„å¤šä¸ªé˜ˆå€¼å°†äº§ç”Ÿå¤šç»„(True Positive, False Positive)ã€‚é’ˆå¯¹æ¯ä¸€ç»„è®¡ç®—<code>precision</code>ä¸<code>recall</code>ä¸¤ä¸ªæŒ‡æ ‡ï¼›</li>\n<li>ä½¿ç”¨step2ä¸­å¾—åˆ°çš„å¤šç»„(precision, recall)ï¼Œä»¥recallä¸ºæ¨ªè½´ï¼Œprecisionä¸ºçºµè½´ï¼Œç”»å‡ºä¸€æ¡æ›²çº¿ã€‚</li>\n</ol>\n<p>ç›®æ ‡æ£€æµ‹æœ¬è´¨ä¸ŠåŒ…å«å¤šåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ä¸”ä¹‹åmAPçš„è®¡ç®—ä¸ä¸Šè¿°å¾—åˆ°PRæ›²çº¿çš„è¿‡ç¨‹ç±»ä¼¼ã€‚</p>\n<h2 id=\"IOU\">IOU</h2>\n<p>mAPçš„è®¡ç®—éœ€è¦IOUçš„å‚ä¸ï¼Œå› æ­¤è¿™ä¸€éƒ¨åˆ†ä»‹ç»IOUçš„è®¡ç®—æ–¹å¼ã€‚ç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨é¢„æµ‹é˜¶æ®µä¼šä¸ºå›¾ç‰‡æˆ–è§†é¢‘ä¸­æ£€æµ‹å‡ºçš„ç‰©ä½“ï¼ˆobjectï¼‰æ ‡è®°Bounding Boxï¼ŒåŒæ—¶è¿˜æœ‰æ¦‚ç‡å€¼ï¼ˆconfidenceï¼‰ã€‚</p>\n<p>IOUçš„è®¡ç®—éœ€è¦Bounding Boxä¸Ground Truth labelå‚ä¸ï¼Œå‰è€…æ˜¯æ¨¡å‹é¢„æµ‹çš„å€¼ï¼Œåè€…è¢«è®¤ä¸ºæ˜¯å›¾ç‰‡ä¸­å±äºç‰©ä½“çš„çœŸå®èŒƒå›´ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè“è‰²ä»£è¡¨Bounding Boxï¼Œç»¿è‰²ä»£è¡¨Ground Truth Boxã€‚</p>\n<p><img src=\"https://www.learnopencv.com/wp-content/uploads/2017/10/object-recognition-false-positives-true-positives.jpg\" alt=\"Region Proposalsç¤ºæ„\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">Region Proposals Example, from https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/</center>\n<p>IOUç”¨äºæè¿°ä¸¤ä¸ªBBoxé—´çš„äº¤é›†ç¨‹åº¦ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸€ä¸ªBBoxå’ŒGround Truth Boxå­˜åœ¨ç›¸äº¤ï¼ˆç»¿è‰²éƒ¨åˆ†ï¼‰ã€‚<br>\n<img src=\"/images/ObjectDetection/IOU.png\" alt=\"ä¸¤ä¸ªBBoxç¤ºæ„\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">ä¸¤ä¸ªBBoxç¤ºæ„</center>\n<p>æ­¤å¤„IOUçš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š<br>\n$$<br>\nIOU = \\frac{Area(ç»¿è‰²ï¼Œoverlap)}{Area(è“+ç»¿+çº¢ï¼Œunion)}<br>\n$$<br>\nç”±äºBBoxä¼šç»™å®š4ä¸ªå€¼ç¡®å®šä½ç½®ï¼Œæ‰€ä»¥è®¡ç®—é¢ç§¯è¿˜æ˜¯å¾ˆå®¹æ˜“çš„ã€‚</p>\n<h2 id=\"mAP\">mAP</h2>\n<p>æœ¬éƒ¨åˆ†ä»¥å•å¼ å›¾ç‰‡ä¸­å•ä¸ªç±»åˆ«æ‰€æœ‰é¢„æµ‹å‡ºçš„BBoxä¸Ground Truth Boxä¸ºä¾‹ï¼Œä»‹ç»mAPçš„è®¡ç®—æµç¨‹ã€‚</p>\n<ol>\n<li>\n<p>é€‰å–ä¸€ä¸ªé˜ˆå€¼æ¯”å¦‚0.5ï¼Œä½¿ç”¨å®ƒå¯¹è¯†åˆ«å‡ºæ¥çš„BBoxè¿›è¡Œåˆ¤æ–­ã€‚å¦‚æœè¯¥BBoxçš„<code>IOU &gt; threshold</code>ï¼Œé‚£ä¹ˆè®¤ä¸ºè¿™ä¸ªBBoxæ˜¯<code>TP</code>ï¼Œå¦åˆ™ï¼Œè®¤ä¸ºå®ƒæ˜¯<code>FP</code>ï¼›</p>\n</li>\n<li>\n<p>æŒ‰ç…§confidenceé™åºæ’åºï¼Œå¾—åˆ°ä¸€ä¸ªè¡¨æ ¼ï¼›</p>\n</li>\n<li>\n<p>è‡ªé¡¶å‘ä¸‹ï¼Œé€šè¿‡ç´¯åŠ è®¡ç®—<code>precision</code>å’Œ<code>recall</code>ï¼Œå¹¶è·å¾—å¤šç»„æ•°æ®ï¼›</p>\n</li>\n<li>\n<p>ä½¿ç”¨step3è·å¾—çš„æ•°æ®ç”»å‡ºPR-curveï¼Œæ³¨æ„æœ€å¼€å§‹åº”è¯¥æ˜¯ä»(0, 1)å¼€å§‹çš„ï¼Œå®ƒä¸åæ ‡è½´å›´æˆçš„é¢ç§¯ï¼Œå³æ˜¯éœ€è¦è®¡ç®—çš„APã€‚</p>\n</li>\n</ol>\n<h2 id=\"ä¸ªäººç–‘æƒ‘\">ä¸ªäººç–‘æƒ‘</h2>\n<p>åˆ†ç±»ä»»åŠ¡ä¸­çš„PRæ›²çº¿æ˜¯ç”±<code>å¤šç»„é˜ˆå€¼</code>è®¡ç®—å‡ºå¤šç»„precisionå’Œrecallå€¼ç”»å‡ºçš„ï¼Œä½†æ˜¯mAPä¸­çš„PR-curveæ˜¯é€šè¿‡<code>ä¸€ä¸ªé˜ˆå€¼</code>è®¡ç®—å‡ºå¤šç»„precisionå’Œrecallå€¼ç”»å‡ºçš„ã€‚ä¸çŸ¥é“æ˜¯æˆ‘å“ªé‡Œç†è§£é”™äº†ï¼Œè¿˜æ˜¯åœ¨mAPä¸­å°±æ˜¯å¦‚æ­¤è§„å®šã€‚</p>\n<h2 id=\"Reference\">Reference</h2>\n<ul>\n<li>\n<p><a href=\"https://www.youtube.com/watch?v=FppOzcDvaDI&amp;list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq&amp;index=4\">https://www.youtube.com/watch?v=FppOzcDvaDI&amp;list=PLhhyoLH6Ijfw0TpCTVTNk42NN08H6UvNq&amp;index=4</a> ï¼Œå¾ˆæœ‰æ‰çš„Youtuberï¼Œè‹±è¯­å‘éŸ³ä¹Ÿå¾ˆå¥½ï¼Œå‘å¸ƒçš„è§†é¢‘éƒ½æ˜¯ä»0åˆ°1å»å®ç°ï¼Œéå¸¸é€‚åˆå¸Œæœ›æ·±åº¦äº†è§£çš„åŒå­¦</p>\n</li>\n<li>\n<p><a href=\"https://github.com/rafaelpadilla/Object-Detection-Metrics\">https://github.com/rafaelpadilla/Object-Detection-Metrics</a> ï¼Œæ–‡å­—ç‰ˆè®¡ç®—è¿‡ç¨‹</p>\n</li>\n</ul>\n","categories":["Computer Vision"],"tags":["Object Detection","Metric"]},{"title":"BatchNormalization and LayerNormalization","url":"/posts/BatchNormalization%20and%20LayerNormalization/","content":"<blockquote>\n<p>This post is to introduce Batch Normalization and Layer Normaliztion, which are of the $\\textit{regularization}$ methods in $\\textit{Deep Learning}$.</p>\n</blockquote>\n<a id=\"more\"></a>\n<h2 id=\"Regularization\">Regularization</h2>\n<p>Deep learning can be very powerful since the stacked deeper layers. Thus itâ€™s easy to overfitt and has poor performance on unseen data. To avoid that, several regularization methods are been proposed. And this post will focus on two methods, namely $\\textrm{Batch Normalization}$ and $\\textrm{Layer Normalization}$.</p>\n<h2 id=\"Batch-Normalization\">Batch Normalization</h2>\n<p>Letâ€™s assume that we want to train a fully connected neural network, and we add a batch normalization layer into the net. We insert it between the activation layer and input layer like follow figure.</p>\n<p><img src=\"/images/Tricks/batch_norm.png\" alt=\"batch norm\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">Figure1</center>\n<p><strong>Firstly, weâ€™ll discuss the train period.</strong></p>\n<p>As the name implies, we need to calculate <code>mean</code> value and <code>standard variation</code> value for each batch train data. Apparently, these two variables are <code>vector</code>. Assume previous layer has 100 neurons, then the mean and std are both 100 dimension.</p>\n<p>The input shape of bacth normalization layer is (batch_size, 100), and use <code>X</code> to represent. So, when training with batch data, we do follow calculation in normalization layer for each batch. <code>i</code> is the number of batch, and the computation is element wise.</p>\n<p>$$<br>\nX_i^{\\prime} = \\frac{X - \\mu_i}{\\sigma_i}<br>\n$$</p>\n<p>After that, we use another two values $\\gamma$ and $\\beta$ to do another calculation. These two parameters are learnable, which means that theyâ€™ll be fitted with data in the training period like parameters <code>W</code> and <code>b</code>. The calculation is as follow:</p>\n<p>$$<br>\nX_i^{\\prime \\prime} = \\gamma X_i^{\\prime} + \\beta<br>\n$$</p>\n<p>$X_i^{\\prime \\prime}$ will be sent to next layer doing $WX_i^{\\prime \\prime} + b$.</p>\n<p>Now, how does batch normalization do during test period? There has no <code>mean</code> value and <code>std</code> for test data, since no batch. What we do is to use the $\\mu_i$ and $\\sigma_i$, which are calculated in batch train period time. Use these values to do weight average. Bigger the batch number is, higher the corresponding weight is.</p>\n<h2 id=\"Layer-Normalization\">Layer Normalization</h2>\n<p>Unlike batch normalization using each batch data to estimate $\\mu$ and $\\sigma$, layer normalization use the units of a layer. The mean value is the avearge of one layerâ€™s units value, so as the standard variation. The equation is as follow:<br>\n$$<br>\n\\mu_i = \\frac{\\sum_{j=1}^H a_H}{H} \\\\<br>\n$$</p>\n<p>Other setting and steps are same as before.</p>\n<h2 id=\"Reference\">Reference</h2>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=BZh1ltr5Rkg\">https://www.youtube.com/watch?v=BZh1ltr5Rkg</a></li>\n</ul>\n"},{"title":"Connectionist Temporal Classification","url":"/posts/Connectionist%20Temporal%20Classification/","content":"<blockquote>\n<p>ç”±äºéœ€è¦è¿›è¡Œå›¾ç‰‡æ–‡æœ¬è¯†åˆ«ç›¸å…³çš„å·¥ä½œï¼Œæœ€è¿‘æ¥è§¦åˆ°äº†CTC Lossï¼Œæ‰“ç®—å°†è¿™å‡ å¤©çš„å­¦ä¹ è¿›è¡Œå½’çº³æ€»ç»“ã€‚å¦‚æœæœ‰æ¥è§¦è¿‡<code>HMM</code>çš„ç›¸å…³å†…å®¹ï¼Œçœ‹åˆ°<code>CTC</code>å¯èƒ½ä¼šè§‰å¾—å¾ˆäº²åˆ‡ã€‚</p>\n</blockquote>\n<a id=\"more\"></a>\n<h2 id=\"1-å‰è¨€\">1. å‰è¨€</h2>\n<p>å¦‚æœæƒ³è¦è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå®ƒä»¬åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œç”¨äºæ–‡æœ¬è¯†åˆ«å·¥ä½œçš„ç¥ç»ç½‘ç»œä¼šç”¨åˆ°<code>convolutional layers</code>å’Œ<code>recurrent layers</code>ï¼Œå‰è€…å¸¸è¢«ç”¨äºæå–å›¾ç‰‡ä¸­çš„ç‰¹å¾å¹¶é€å…¥åè€…ã€‚åè€…çš„è¾“å‡ºæ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå­˜æ”¾ç€æ¯ä¸ªæ—¶é—´æ­¥ä¸­å±äºæŸä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚åŸºäºCRNNæ¶æ„çš„æ–‡å­—è¯†åˆ«å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ</p>\n<p><img src=\"/images/ObjectDetection/CTC.png\" alt=\"CRNN Overview\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">Figure1, CRNN Overview</center>\n<p>è¦è®­ç»ƒè¿™æ ·çš„ç½‘ç»œï¼Œå¯ä»¥è®¾æƒ³ä¸€ä¸‹æœ€é€šä¿—çš„åšæ³•ï¼Œé‚£å°±æ˜¯å­—ç¬¦çº§åˆ«çš„åˆ†ç±»è¯†åˆ«ã€‚æ¯”å¦‚å¯¹äºä¸Šå›¾æ¥è¯´ï¼ŒåŒæ—¶éœ€è¦æ ‡æ³¨å›¾ç‰‡ä¸­æ¯ä¸ªå­—ç¬¦çš„ä½ç½®ï¼Œæ¯”å¦‚å›¾ç‰‡ä¸­<code>A</code>æ‰€åœ¨çš„ä½ç½®ï¼Œç„¶åè¿™ä¸ªéƒ¨åˆ†å¯¹åº”çš„å­—ç¬¦æ˜¯<code>A</code>ï¼Œä¾æ¬¡ç±»æ¨ï¼Œå¯¹è¿™å¼ å›¾ç‰‡é‡Œçš„å…¶ä»–å­—ç¬¦è¿˜éœ€è¦åšåŒæ ·æ ‡æ³¨è¡Œä¸ºã€‚å¯¹äºå›¾ç‰‡ï¼Œè¿™æ ·æ ‡æ³¨ä»…ä»…åªæ˜¯è€—æ—¶ï¼Œä½†æ˜¯æœ‰äº›ä»»åŠ¡æ¯”å¦‚è¯­éŸ³è¯†åˆ«ï¼Œç”šè‡³æ— æ³•æ ‡æ³¨å­—ç¬¦å‡ºç°çš„ä½ç½®ã€‚</p>\n<p>ä¸ºäº†è§£å†³ä¸Šè¿°éš¾é¢˜ï¼Œ<a href=\"https://www.cs.toronto.edu/~graves/icml_2006.pdf\">CTC</a>è¢«æå‡ºäº†ï¼Œå®ƒæ˜¯ä¸€ç§èƒ½è®©RNNç›´æ¥åˆ©ç”¨å›¾ç‰‡æ•°æ®æˆ–è¯­éŸ³æ•°æ®ä¸æ–‡æœ¬æ•°æ®è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„æ–¹æ³•ã€‚</p>\n<h2 id=\"2-å¯¹é½\">2. å¯¹é½</h2>\n<p>ä¸æ ‡æ³¨å­—ç¬¦çš„å…·ä½“ä½ç½®ï¼Œå¯¹äºåƒå›¾ç‰‡æ•°æ®è½¬æ–‡å­—æˆ–è€…è¯­éŸ³æ•°æ®è½¬æ–‡å­—çš„ä»»åŠ¡ï¼Œä¼šå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯å¦‚ä½•å°†è¾“å…¥ä¸è¾“å‡º<code>å¯¹é½</code>ã€‚</p>\n<p>æ‹¿å›¾ç‰‡æ¥è¯´ï¼Œè¾“å…¥å›¾ç‰‡ç»è¿‡CNNåè½¬åŒ–å¾—åˆ°ç‰¹å¾å‘é‡ï¼Œå°†è¿™äº›ç‰¹å¾ç»„æˆåºåˆ—è¾“å…¥RNNã€‚ä¸ºäº†æ–¹ä¾¿ï¼ŒRNNçš„åºåˆ—æ­¥é•¿<code>T</code>æ˜¯å›ºå®šçš„ï¼Œç›¸åº”çš„ï¼ŒRNNçš„è¾“å‡ºä¹Ÿæ˜¯<code>T</code>ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸èƒ½ä¿è¯æ•°æ®é›†<code>&lt;X, Y&gt;</code>ä¸­çš„æ–‡å­—é•¿åº¦éƒ½æ˜¯<code>T</code>ï¼ŒRNNçš„è¾“å…¥ä¸æ ‡ç­¾   <code>y</code>å¯èƒ½æ— æ³•å¯¹é½ã€‚æ¯”å¦‚åœ¨å›¾ç‰‡<code>Figure1</code>ä¸­ï¼Œè®¾<code>T = 10</code>ï¼Œè€Œç›®æ ‡å•è¯<code>Available</code>çš„é•¿åº¦åªæœ‰9ï¼Œæ­¤æ—¶å°±æ— æ³•å¯¹é½ã€‚</p>\n<p>ä¸ºäº†è§£å†³å¯¹é½é—®é¢˜ï¼Œä¸º<code>Figure1</code>å¢åŠ <code>CTC Layer</code>ï¼ŒåŠ åœ¨RNNè¾“å‡ºä¹‹åã€‚</p>\n<p><img src=\"/images/ObjectDetection/CTC_decoder.png\" alt=\"CRNN Overview\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">Figure2, CRNN with decoder</center>\n<p>å¦‚<code>Figure2</code>æ‰€ç¤ºï¼Œè®¾RNNè¾“å‡ºçš„æ­¥é•¿å›ºå®šä¸º10ã€‚å½“$y_i$çš„é•¿åº¦ä¸è¶³10æ—¶ï¼Œå…è®¸RNNèƒ½åœ¨ç›¸é‚»ä½ç½®ä¸Šé¢„æµ‹å‡ºç›¸åŒçš„å­—ç¬¦ã€‚åœ¨<code>CTC layer</code>ä¸­è§£ç æ—¶ï¼Œå°†ç›¸é‚»ç›¸åŒçš„å­—ç¬¦åˆå¹¶å³å¯ã€‚</p>\n<p>ä½†æ˜¯ï¼Œåˆæ¥äº†ä¸€ä¸ªé—®é¢˜ï¼Œå¦‚æœ$y_i == hello$ï¼Œå³æ ‡ç­¾ä¸­ç›¸é‚»ä½ç½®æœ¬å°±å­˜åœ¨ç›¸åŒå­—ç¬¦ï¼Œä¸Šè¿°çš„è§£ç ç­–ç•¥å°±è¡Œä¸é€šäº†ã€‚ä¸ºæ­¤ï¼Œå†å¼•å…¥ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦$\\epsilon$ï¼Œç§°ä½œ<code>blank token</code>ï¼Œåœ¨è§£ç çš„æ—¶å€™å»æ‰ã€‚å¦‚æœRNNåºåˆ—é•¿åº¦è¿˜æ˜¯10ï¼Œé‚£ä¹ˆæ¯”å¦‚RNNè¾“å‡ºè·¯å¾„ä¸º$h e l \\epsilon l \\epsilon o o o o$ï¼Œæ˜¯èƒ½è¢«è§£ç åˆ°<code>hello</code>çš„ã€‚</p>\n<p>æ€»ç»“æ¥è¯´ï¼Œä¸ºäº†èƒ½å¤Ÿè§£å†³<strong>å¯¹é½é—®é¢˜</strong>ï¼Œæˆ‘ä»¬åœ¨<code>RNN layer</code>ååŠ äº†<code>CTC layer</code>ï¼Œå¹¶ä¸”ä¸º<code>RNN</code>çš„å­—ç¬¦åˆ†ç±»å¢åŠ æ–°çš„ä¸€ç±»$\\epsilon$ã€‚æ ¹æ®ç›¸åº”çš„è§£ç è§„åˆ™ï¼ŒæœŸå¾…<code>RNN</code>çš„è¾“å‡ºåœ¨ç»è¿‡<code>CTC</code>çš„è½¬åŒ–åèƒ½å¾—åˆ°æ­£ç¡®è·¯å¾„ã€‚æ›´åŠ å…·ä½“çš„è§£ç ç»†èŠ‚ï¼Œä¼šæ”¾åœ¨<code>ç¬¬4éƒ¨åˆ†æ¦‚ç‡è®¡ç®—</code>ä¸­ä»‹ç»ã€‚</p>\n<h2 id=\"3-ç›®æ ‡å‡½æ•°\">3. ç›®æ ‡å‡½æ•°</h2>\n<p>ç»ˆäºæ¥åˆ°æœ¬æ–‡çš„é‡ç‚¹ï¼Œè¿™ä¸€éƒ¨åˆ†å°†ä»‹ç»<code>CTC Loss</code>ã€‚</p>\n<p>åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬çŸ¥é“RNNè¦å¤„ç†çš„æ˜¯å¤šå¯¹å¤šçš„å¤šåˆ†ç±»é—®é¢˜ï¼Œå®ƒéœ€è¦å°†è¾“å…¥çš„ç‰¹å¾åºåˆ—è½¬åŒ–ä¸ºåºåˆ—çŸ©é˜µã€‚çŸ©é˜µçš„æ¯ä¸€è¡Œæ˜¯å­—ç¬¦æ‰€å¯¹åº”çš„ç±»åˆ«ï¼ŒçŸ©é˜µçš„æ¯ä¸€åˆ—æ˜¯æ‰€å¤„çš„æ—¶é—´æ­¥ã€‚</p>\n<h4 id=\"3-1-æ¦‚ç‡\">3.1. æ¦‚ç‡</h4>\n<p>ä¸ºäº†æ›´åŠ å½¢è±¡ï¼Œä¸‹é¢ä»¥å…·ä½“çš„ä»»åŠ¡è¿›è¡Œä¸¾ä¾‹ã€‚å‡è®¾å¾…é¢„æµ‹çš„å›¾ç‰‡åªåŒ…æ‹¬<code>æˆ‘çˆ±å­¦ä¹ </code>4ä¸ªå­—ï¼Œ<code>RNN</code>åœ¨æ¯ä¸ªæ—¶é—´æ­¥éœ€è¦é¢„æµ‹çš„å­—ç¬¦ä¹Ÿä»…æœ‰5ç±»$\\{æˆ‘ï¼Œçˆ±ï¼Œå­¦ï¼Œä¹ ï¼Œ\\epsilon\\}$ï¼ŒRNNçš„åºåˆ—æ­¥é•¿å›ºå®šä¸º5ã€‚é‚£ä¹ˆRNNçš„ä¸€æ¬¡è¾“å‡ºå¯èƒ½äº§ç”Ÿå¦‚ä¸‹çŸ©é˜µï¼š</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\"></th>\n<th style=\"text-align:center\">$step_1$</th>\n<th style=\"text-align:center\">$step_2$</th>\n<th style=\"text-align:center\">$step_3$</th>\n<th style=\"text-align:center\">$step_4$</th>\n<th style=\"text-align:center\">$step_5$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><strong>æˆ‘</strong></td>\n<td style=\"text-align:center\">0.469</td>\n<td style=\"text-align:center\">0.202</td>\n<td style=\"text-align:center\">0.244</td>\n<td style=\"text-align:center\">0.011</td>\n<td style=\"text-align:center\">0.154</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>çˆ±</strong></td>\n<td style=\"text-align:center\">0.043</td>\n<td style=\"text-align:center\">0.217</td>\n<td style=\"text-align:center\">0.211</td>\n<td style=\"text-align:center\">0.314</td>\n<td style=\"text-align:center\">0.185</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>å­¦</strong></td>\n<td style=\"text-align:center\">0.249</td>\n<td style=\"text-align:center\">0.179</td>\n<td style=\"text-align:center\">0.207</td>\n<td style=\"text-align:center\">0.152</td>\n<td style=\"text-align:center\">0.204</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>ä¹ </strong></td>\n<td style=\"text-align:center\">0.144</td>\n<td style=\"text-align:center\">0.13</td>\n<td style=\"text-align:center\">0.137</td>\n<td style=\"text-align:center\">0.296</td>\n<td style=\"text-align:center\">0.227</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><strong>$\\epsilon$</strong></td>\n<td style=\"text-align:center\">0.096</td>\n<td style=\"text-align:center\">0.272</td>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.227</td>\n<td style=\"text-align:center\">0.231</td>\n</tr>\n</tbody>\n</table>\n<p>æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæ‰€æœ‰å­—ç¬¦å‡ºç°çš„æ¦‚ç‡å’Œä¸º1ã€‚ä¸‹é¢ï¼Œå¯ä»¥é€šè¿‡è¿™ä¸ªçŸ©é˜µè®¡ç®—å‡ºè§£ç å±‚æœ€åè¾“å‡º<code>æˆ‘çˆ±å­¦ä¹ </code>çš„æ¦‚ç‡ã€‚é¦–é€‰éœ€è¦åˆ—å‡ºæ‰€æœ‰å¯èƒ½è§£ç å‡º<code>æˆ‘çˆ±å­¦ä¹ </code>çš„è·¯å¾„ï¼Œä½†æ˜¯å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå®é™…çš„è§£ç è¿‡ç¨‹ä¸­å¹¶ä¸ä¼šé€šè¿‡è¿™ç§ç©·ä¸¾æ–¹æ³•å¾—åˆ°éœ€è¦çš„æ¦‚ç‡ï¼Œå› ä¸ºè®¡ç®—é‡éå¸¸çš„å¤§ã€‚</p>\n<p><strong>è·¯å¾„ï¼š</strong></p>\n<ol>\n<li>$\\epsilon$ -&gt; æˆ‘ -&gt; çˆ± -&gt; å­¦ -&gt; ä¹ </li>\n<li>æˆ‘ -&gt; $\\epsilon$ -&gt; çˆ± -&gt; å­¦ -&gt; ä¹ </li>\n<li>æˆ‘ -&gt; çˆ± -&gt; $\\epsilon$ -&gt; å­¦ -&gt; ä¹ </li>\n<li>æˆ‘ -&gt; çˆ± -&gt; å­¦ -&gt; $\\epsilon$ -&gt; ä¹ </li>\n<li>æˆ‘ -&gt; çˆ± -&gt; å­¦ -&gt; ä¹  -&gt; $\\epsilon$</li>\n<li>æˆ‘ -&gt; æˆ‘ -&gt; çˆ± -&gt; å­¦ -&gt; ä¹ </li>\n<li>æˆ‘ -&gt; çˆ± -&gt; çˆ± -&gt; å­¦ -&gt; ä¹ </li>\n<li>æˆ‘ -&gt; çˆ± -&gt; å­¦ -&gt; å­¦ -&gt; ä¹ </li>\n<li>æˆ‘ -&gt; çˆ± -&gt; å­¦ -&gt; ä¹  -&gt; ä¹ </li>\n</ol>\n<p>ä¸Šé¢åˆ—å‡ºäº†æ‰€æœ‰èƒ½å¾—åˆ°æ­£ç¡®è§£ç åºåˆ—çš„è·¯å¾„ï¼Œå…±æœ‰9æ¡ã€‚è·¯å¾„æ•°é‡ä¼šå—åˆ°RNNåºåˆ—é•¿åº¦ã€æ ‡ç­¾æ–‡å­—é•¿åº¦ä»¥åŠæ ‡ç­¾æ–‡å­—ä¸­é‡å¤å­—ç¬¦çš„ä¸ªæ•°çš„å½±å“ï¼Œè€Œä¸”æ˜¯æŒ‡æ•°çº§åˆ«çš„å¢é•¿ã€‚</p>\n<p>æ¥ä¸‹æ¥ï¼Œéœ€è¦åˆ†åˆ«è®¡ç®—ä¸Šè¿°è·¯å¾„çš„æ¦‚ç‡ï¼Œä¹‹åå†æ±‚å’Œï¼Œå³å¯å¾—åˆ°æ¨¡å‹è¾“å‡º<code>æˆ‘çˆ±å­¦ä¹ </code>çš„æ¦‚ç‡ã€‚</p>\n<p>\\begin{equation}<br>\n\\tag{1}<br>\np(æˆ‘çˆ±å­¦ä¹ |x) = \\sum_{j=1}^{m=9} p(path^j|x) = \\sum_{j=1}^{9} (\\prod_{t=1}^5 p(z_t^j|x))<br>\n\\end{equation}</p>\n<h4 id=\"3-2-æœ€å¤§ä¼¼ç„¶ä¼°è®¡\">3.2. æœ€å¤§ä¼¼ç„¶ä¼°è®¡</h4>\n<blockquote>\n<p>å‚æ•°çš„æ¦‚ç‡ä¼°è®¡é—®é¢˜æ€»æ˜¯ç»•ä¸è¿‡<code>MLE</code>ã€‚</p>\n</blockquote>\n<p>å‰é¢å·²ç»ä»¥è®¡ç®—$p(æˆ‘çˆ±å­¦ä¹ |x)$ä½œä¸ºæ¦‚ç‡è®¡ç®—çš„ä¸¾ä¾‹ã€‚å‡å®šç°åœ¨æœ‰ä¸€ç»„æ•°æ®é›†ï¼ŒåŒ…æ‹¬$N$ç»„å›¾ç‰‡ä¸æ–‡å­—$\\left&lt;x_n, y_n\\right&gt;$çš„å¯¹åº”å…³ç³»ï¼Œè®°ä½œ$\\left&lt;X, Y\\right&gt;$ã€‚åŒæ ·çš„ï¼Œåˆ†åˆ«è®¡ç®—å‡ºè¿™äº›æ ·æœ¬äº§å‡ºçš„æ¦‚ç‡ï¼ˆå†æ¬¡å¼ºè°ƒï¼Œä¸ä¼šç©·ä¸¾è®¡ç®—ï¼Œå…·ä½“ç®—æ³•æ–‡ç« ä¹‹åä¼šä»‹ç»ï¼‰ï¼Œå¾—åˆ°è‹¥å¹²ç»„æ¦‚ç‡$p(y_n|x_n)$ã€‚</p>\n<p>æ ¹æ®æå¤§ä¼¼ç„¶çš„æ€æƒ³ï¼Œ<strong>å¥½çš„æ¨¡å‹ï¼Œæˆ–è€…è¯´å¥½æ¨¡å‹çš„å‚æ•°</strong>ä¼šä»¤ä¼¼ç„¶å‡½æ•°æœ€å¤§ï¼Œå¸Œæœ›æ ¹æ®ä¸‹å¼å¾—åˆ°æ¨¡å‹çš„å‚æ•°ï¼ˆä¸ºäº†ä¹¦å†™ç®€ä¾¿$\\theta$æœ‰æ—¶çœç•¥ï¼‰ï¼Œæœ‰ï¼š</p>\n<p>\\begin{equation}<br>\n\\tag{2}<br>\n\\theta^* = \\underset{\\theta}{\\mathrm{argmin}} \\prod_{n=1}^N  p(y_n|x_n, \\theta)<br>\n\\end{equation}</p>\n<p>åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†<code>CTC Loss</code>çš„ç›®æ ‡å‡½æ•°(Objective function)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p>\n<p>\\begin{equation*}<br>\n\\tag{3}<br>\n\\label{a}<br>\n\\max \\prod_{n=1}^N  p(y_n|x_n) \\iff \\max \\sum_{n=1}^N \\log p(y_n|x_n) \\<br>\n\\iff \\min -\\sum_{n=1}^N  \\log p(y_n|x_n)<br>\n\\end{equation*}</p>\n<p>å¦‚$\\eqref{a}$å¼æ‰€ç¤ºï¼Œç›®æ ‡æ˜¯<code>æœ€å°åŒ–negative log-likelihood</code>ã€‚</p>\n<h2 id=\"4-æ¦‚ç‡è®¡ç®—\">4. æ¦‚ç‡è®¡ç®—</h2>\n<p>åœ¨<code>3.1</code>ä¸­æˆ‘ä»¬ç”¨ç©·ä¸¾çš„æ–¹æ³•åˆ—å‡ºäº†æ‰€æœ‰èƒ½å¾—åˆ°æ–‡æœ¬<code>æˆ‘çˆ±å­¦ä¹ </code>çš„å¯èƒ½è·¯å¾„ï¼Œä¸‹é¢å°†ä»‹ç»<code>CTC</code>çš„å¦ä¸€ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼Œå³å¦‚ä½•ä¼˜åŒ–æ¦‚ç‡è®¡ç®—ç®—æ³•ã€‚</p>\n<p>CTCä¸­ç”¨åˆ°æ¦‚ç‡è®¡ç®—ç®—æ³•ä¸HMMçš„å‰å‘-åå‘ç®—æ³•å¾ˆåƒï¼Œè®ºæ–‡ä¸­ç§°ä½œ<code>CTC Forward-Backward Algorithm</code>ï¼Œä¸‹é¢ä»¥è¯¥ç§°å‘¼æŒ‡ä»£ã€‚å‰å‘-åå‘ç®—æ³•å…³æ³¨çš„ç‚¹åœ¨äºå¦‚ä½•åˆ©ç”¨ç›¸é‚»æ—¶é—´æ­¥çŠ¶æ€çš„å…³ç³»ç®€åŒ–è®¡ç®—é‡ã€‚</p>\n<p>è®ºæ–‡ç»™å‡ºå‰å‘æ¦‚ç‡çš„å®šä¹‰å¦‚ä¸‹ï¼š</p>\n<p>\\begin{equation}<br>\n\\tag{4}\\label{b}<br>\np(y_1, y_2, \\cdots, y_s|x) = \\alpha_{t}(s) \\stackrel{def}{=} \\sum_{\\pi \\in N^{T}: \\atop \\mathcal{B}\\left(\\pi_{1:t}\\right)=label_{1:s}} {} \\prod_{t^{\\prime}=1}^{t} y_{\\pi_{t^{\\prime}}}^{t^{\\prime}}<br>\n\\end{equation}</p>\n<p>å…¶ä¸­ï¼š</p>\n<ul>\n<li>$\\pi$æ˜¯å•æ¡å¯èƒ½çš„è·¯å¾„ï¼Œ$N^{T}$æ˜¯æ‰€æœ‰çš„è·¯å¾„ï¼Œ$T$æ˜¯RNNçš„è¾“å…¥(è¾“å‡º)æ­¥é•¿ï¼›</li>\n<li>$\\mathcal{B}$æ˜¯è§£ç çš„è§„åˆ™ï¼Œå°±åƒå‰é¢è¯´çš„é‚£æ ·ï¼Œ<code>åˆ é™¤é‡å¤å­—ç¬¦</code>ä¸<code>å»é™¤blank token</code>ï¼›</li>\n<li>$t$ä»£è¡¨è·¯å¾„ä¸­çš„ç¬¬å‡ ä¸ªæ—¶é—´æ­¥ï¼Œ$s$ä»£è¡¨æ ‡ç­¾çš„ç¬¬å‡ ä¸ªå­—ç¬¦ã€‚</li>\n</ul>\n<p>æ€»ä½“æ¥è¯´ï¼Œ$\\eqref{b}$çœ‹èµ·æ¥éå¸¸æŠ½è±¡ï¼Œä¸‹é¢ç»“åˆå®é™…çš„æ ‡æ³¨æ–‡æœ¬ï¼š$æˆ‘çˆ±å­¦ä¹ $</p>\n<p>å‡å®šå°†RNNçš„è¾“å‡ºæ­¥é•¿$T$å›ºå®šä¸º10ã€‚ç°åœ¨éœ€è¦è®¡ç®—ç¬¬2ä¸ªå­—ç¬¦<code>çˆ±</code>åœ¨æ—¶é—´æ­¥ä¸º5æ—¶çš„å‰å‘æ¦‚ç‡ï¼Œé‚£ä¹ˆå¯ä»¥å†™ä½œ$\\alpha_{5}(\\text{çˆ±})$ã€‚ä¸ºäº†æ›´åŠ æ¸…æ¥šï¼Œåœ¨å®šä¹‰ä¸ŠåŠ ä¸Šå­—ç¬¦çš„ä½ç½®ä¿¡æ¯ï¼Œå†™ä½œ$\\alpha_{5}(label_2=\\text{çˆ±})$</p>\n<p>æ­¤æ—¶ï¼Œ$label_{1:s}$å°±æ˜¯$label_{1:2}$ï¼š$æˆ‘çˆ±$ã€‚æˆ‘ä»¬éœ€è¦æ‰¾åˆ°æ‰€æœ‰çš„$\\pi$ï¼Œä½¿å¾— $\\mathcal{B}(\\pi_{1:5}) == æˆ‘çˆ±$ã€‚</p>\n<p>æ¯”å¦‚ï¼Œå…¶ä¸­ç¬¦åˆè¦æ±‚çš„è·¯å¾„$\\pi$å¯ä»¥æ˜¯ï¼š</p>\n<ul>\n<li>$\\epsilon \\rightarrow \\epsilon \\rightarrow æˆ‘ \\rightarrow \\epsilon \\rightarrow çˆ± \\rightarrow \\cdots$</li>\n<li>$\\epsilon \\rightarrow æˆ‘ \\rightarrow æˆ‘ \\rightarrow \\epsilon \\rightarrow çˆ± \\rightarrow \\cdots$</li>\n<li>$\\epsilon \\rightarrow æˆ‘ \\rightarrow \\epsilon \\rightarrow çˆ± \\rightarrow çˆ± \\rightarrow \\cdots$</li>\n<li>$\\cdots$</li>\n</ul>\n<p>å¦‚ä¸Šæ‰€ç¤ºï¼Œè™½ç„¶è§„å®šè·¯å¾„çš„é•¿åº¦ä¸º10ï¼Œä½†ç”±äºéœ€è¦çš„æ—¶é—´æ­¥$t$åªæœ‰5ï¼Œä»…è€ƒè™‘å‰5ä¸ªå­—ç¬¦ç»„åˆä¹‹åï¼Œç¬¦åˆè§£ç éœ€æ±‚çš„è·¯å¾„ã€‚</p>\n<h4 id=\"4-1-å‰å‘ç®—æ³•\">4.1. å‰å‘ç®—æ³•</h4>\n<p>ä¸‹é¢å…ˆç”¨ä¸€å¼ å›¾æ¥ä»‹ç»å‰å‘è¿‡ç¨‹ä¸­ã€Œå­—ç¬¦çš„æµåŠ¨ã€ã€‚</p>\n<p><img src=\"/images/ObjectDetection/CTC_forward.png\" alt=\"CTC Forward\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">Figure3, CTC Forward</center>\n<p>éœ€è¦æ³¨æ„çš„è¿™å¼ å›¾å¹¶ä¸æ˜¯RNNçš„è¾“å‡ºçŸ©é˜µï¼Œå®é™…çš„è¾“å‡ºçŸ©é˜µè¿˜æ˜¯åªæœ‰5è¡Œï¼Œã€Œæˆ‘ï¼Œçˆ±ï¼Œå­¦ï¼Œä¹ ï¼Œ$\\epsilon$ã€ã€‚è¿™é‡Œåªæ˜¯ä¸ºäº†è®©å‰å‘è¿‡ç¨‹çš„çŠ¶æ€è½¬å˜æ›´å¥½ç†è§£ï¼Œæ‰€ä»¥åšäº†æ·»åŠ $\\epsilon$çš„æ“ä½œã€‚</p>\n<p>$\\epsilon$åœ¨å›¾ä¸­ç”¨<code>-</code>è¡¨ç¤ºã€‚æˆ‘ä»¬å…ˆæ˜¯å°†$\\epsilon$åˆ†åˆ«æ’å…¥æ–‡æœ¬çš„å¤´å°¾ä»¥åŠå†…éƒ¨ï¼Œç„¶åä»å·¦ä¸Šåˆ°å³ä¸‹ï¼Œå°†æ‰€æœ‰å¯èƒ½å¾—åˆ°$æˆ‘çˆ±$çš„è·¯å¾„ç”»äº†å‡ºæ¥ã€‚</p>\n<p>ä¸ºäº†å¥‘åˆä¸Šé¢çš„å˜åŒ–ï¼Œå¯¹äºæ–‡æœ¬ä¹Ÿåšç›¸åº”çš„å˜åŒ–ï¼š$$labelã€Œæˆ‘çˆ±å­¦ä¹ ã€\\rightarrow label^{\\prime} ã€Œ\\epsilonæˆ‘\\epsilonçˆ±\\epsilonå­¦\\epsilonä¹ \\epsilonã€$$</p>\n<p>è¦è·å–$\\alpha_{5}(label_2=\\text{çˆ±})$ï¼Œå¯ä»¥åˆ©ç”¨å…¬å¼(5)ï¼š</p>\n<p>\\begin{equation}<br>\n\\tag{5}<br>\n\\alpha_{5}(label_2=\\text{çˆ±}) = \\alpha_{5}(label^{\\prime}_ 4=\\text{çˆ±}) + \\alpha_{5}(label^{\\prime}_5=\\epsilon)<br>\n\\end{equation}</p>\n<p>è€Œï¼Œ<br>\n\\begin{equation}<br>\n\\begin{aligned}<br>\n\\alpha_{5}(label^{\\prime}_ 4=\\text{çˆ±}) &amp;= \\left(\\alpha_{4}(label^{\\prime}_ 4=\\text{çˆ±}) + \\alpha_{4}(label^{\\prime}_ 3=\\epsilon) + \\alpha_{4}(label^{\\prime}_ 2=\\text{æˆ‘})\\right) * p(label^{\\prime}=\\text{çˆ±}) \\\\<br>\n\\alpha_{5}(label^{\\prime}_ 5=\\epsilon) &amp;= \\left(\\alpha_{4}(label^{\\prime}_ 5=\\epsilon) + \\alpha_{4}(label^{\\prime}_ 4=\\text{çˆ±})\\right) * p(label^{\\prime}=\\epsilon)<br>\n\\end{aligned}<br>\n\\end{equation}</p>\n<br>\n<p>å½“ç¬¬$s$ä¸ª$label^{\\prime}$æ˜¯$\\epsilon$æ—¶ï¼Œæœ‰ä¸¤ä¸ªè½¬æ¢æ¥æºï¼›å½“æ˜¯æ™®é€šå­—ç¬¦æ—¶ï¼Œæœ‰ä¸‰ä¸ªè½¬æ¢æ¥æºã€‚<strong>è¿˜æœ‰ä¸€ä¸ªè¦æ³¨æ„çš„æ˜¯ï¼Œå½“ç›¸é‚»ä¸¤ä¸ªå­—ç¬¦æ˜¯ç›¸åŒçš„æ—¶å€™ï¼Œä¹Ÿåªæœ‰ä¸¤ä¸ªè½¬æ¢æ¥æº</strong>ã€‚å¦‚ä¸‹å›¾ï¼š</p>\n<p><img src=\"/images/ObjectDetection/CTC_forward2.png\" alt=\"CTC Forward2\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">Figure4, CTC Forward 2</center>\n<p>è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºé’ˆå¯¹$label^{\\prime}$çš„è½¬æ¢æ–¹ç¨‹ï¼Œä¸ºäº†ç®€å†™ï¼Œç”¨$l$ä»£æ›¿$label$ï¼š</p>\n<p>\\begin{equation}<br>\n\\alpha_t(l^{\\prime}_s)=<br>\n\\begin{cases}<br>\n\\left[\\alpha _{t-1}(l^{\\prime}_s) + \\alpha _{t-1}(l^{\\prime} _{s-1})\\right] * p(l^{\\prime}_s), \\ \\ if \\ \\ l^{\\prime}_s=\\epsilon \\ \\ or \\ \\ l^{\\prime}_s=l^{\\prime} _{s-2}<br>\n\\\\<br>\n\\\\<br>\n\\left[\\alpha _{t-1}(l^{\\prime}_s) + \\alpha _{t-1}(l^{\\prime} _{s-1}) + \\alpha _{t-1}(l^{\\prime} _{s-2})\\right] * p(l^{\\prime}_s), \\ \\ otherwise<br>\n\\end{cases}<br>\n\\end{equation}</p>\n<h4 id=\"4-2-åå‘ç®—æ³•\">4.2. åå‘ç®—æ³•</h4>\n<p>åå‘ç®—æ³•ä¸å‰å‘ç®—æ³•çš„æ€è·¯ä¸€æ¨¡ä¸€æ ·ï¼Œåœ¨è¿™é‡Œä¸å†èµ˜è¿°ã€‚</p>\n<h2 id=\"Inference-and-Beam-Search\">Inference and Beam Search</h2>\n<p><a href=\"https://zhuanlan.zhihu.com/p/36029811?group_id=972420376412762112\">æœªå¾…å®Œç»­</a></p>\n<h2 id=\"Reference\">Reference</h2>\n<ul>\n<li><a href=\"https://distill.pub/2017/ctc/\">https://distill.pub/2017/ctc/</a></li>\n<li><a href=\"https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c\">https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=UMxvZ9qHwJs\">https://www.youtube.com/watch?v=UMxvZ9qHwJs</a></li>\n</ul>\n","categories":["Computer Vision"],"tags":["Text Recognition","Loss"]},{"title":"Spelling Error Correction with Soft-Masked BERT","url":"/posts/paper_spelling_error_correction/","content":"<blockquote>\n<p>Recently, I want to learn how to build a knowledge graph. In the period, I realize that <code>spelling error correction</code> is one of the most important links. So, I should find a paper, do some reading and take notes about it.</p>\n</blockquote>\n<a id=\"more\"></a>\n<h1>Challenges</h1>\n<p>For the <code>Spelling Error Correction for Chinese Character</code>, there are mainly two challenges, displayed as follow:</p>\n<ul>\n<li>First, some mistaken is produces by written. E.g. :\n<ul>\n<li>Wrong: åŸƒåŠæœ‰é‡‘å­å¡”ã€‚Egypt has golden towers.</li>\n<li>Correct: åŸƒåŠæœ‰é‡‘å­—å¡”ã€‚Egypt has pyramids.</li>\n<li>For this condition, we could correct it with the <code>world knowledge</code>.</li>\n</ul>\n</li>\n<li>Another situation occurs when we need to correct it with <code>inference</code>. E.g:\n<ul>\n<li>Wrong: ä»–çš„æ±‚èƒœæ¬²å¾ˆå¼ºï¼Œä¸ºäº†è¶Šç‹±åœ¨æŒ–æ´ã€‚ He has a strong desire to win and is digging for prison breaks</li>\n<li>Correct: ä»–çš„æ±‚ç”Ÿæ¬²å¾ˆå¼ºï¼Œä¸ºäº†è¶Šç‹±åœ¨æŒ–æ´ã€‚ He has a strong desire to survive and is digging for prison breaks.</li>\n</ul>\n</li>\n</ul>\n<h1>Past Methods</h1>\n<p>It exists mainly two categories methods, namely <code>traditional machine learning</code> and <code>deep learning</code>.</p>\n<p>Some methods are displayed as below:</p>\n<ol>\n<li>a unified framework, which is consists of a pipeline of error detection, candidate generation and final candidate selection by traditional machine learning.</li>\n<li>a <code>Seq2Seq</code> model with copy mechanism which transforms an input sentence into a new sentence with spelling errors corrected.</li>\n<li>Bert based model. The dataset to fine-tuning the Bert can be generated by <code>a large confusion table</code>. In the inference period, Bert predict the most probability character for each position.</li>\n</ol>\n<p>In the above methods, we can get awesome accuracy with <code>Bert</code>. However, it seems that the result could be better with some change. In the origin bert, the model randomly select 15% words to mask, which results the model only learn the distribution of masked token and choose not to make any correction.</p>\n<h1>Proposed method</h1>\n<p>The proposed method in this paper is also Bert based. To address the aforementioned issue, the proposed model contains two network, which is detection network and another is correction network.</p>\n<p>Below is the architecture of proposed model: <code>Soft-Masked BERT</code></p>\n<p><img src=\"/images/soft-masked-bert-arch.jpg\" alt=\"model architecture\"><center style=\"font-size:14px;color:#C0C0C0\">model architecture</center></p>\n<p>As the figure illustrates, the model mainly contains two network. The correction network is <code>a Bi-GRU</code> network that predicts the probability whether the character is error for each position. And the correction network is the same as former Bert.</p>\n<p><strong>Next, weâ€™ll dive into the detail of training method of the <code>Soft-Masked BERT</code>.</strong></p>\n<ol>\n<li>Firstly, we should creates an embedding for each character in the input sentence, which is referred as the <code>input embedding</code>.</li>\n<li>Next, we send the input embedding to <code>detection network</code> and get the output of probability of errors for the character in each position.</li>\n<li>Now, we have the probability indicating whether itâ€™s error in each position. We do weighted sum for the <code>input embedding</code> and <code>[MASK] embedding</code> by the error probabilities.\n<ul>\n<li><strong>To explain clearly, we can look the <code>architecture illustration</code> above. We take the <code>3rd position</code> for the example.</strong></li>\n<li>With the detection network, we get the error probability of this position, denoted as $p_3$, which indicates how much the character in this position would be error. Therefore, the correct probability of the position is $1 - p_3$.</li>\n<li>Meanwhile, we have the embedding of the <code>3rd position character</code> and <code>[MASK] token</code>. So we can do weighted sum for these two embedding, and the weight is just the $p_3$ and $1 - p_3$.</li>\n</ul>\n</li>\n<li>Use the output of detection model as the input of the next correction model.</li>\n<li>There is also a residual connection between the input embedding and the output of correction model. The combination of them will be sent to the softmax layer to predict the max probability of the character, which should be put on this position.</li>\n</ol>\n<h2 id=\"Training\">Training</h2>\n<p>This model is training end-to-end, although it contains two sub-model.</p>\n<p>And the objective function for these two task are both <code>cross entropy</code>. To learning on a better way, this paper use a coefficient to combine these two loss, which is described as below:</p>\n<p>\\begin{equation}<br>\n\\boldsymbol \\ell_{total} = \\lambda \\cdot \\boldsymbol \\ell_{c}+(1-\\lambda) \\cdot \\boldsymbol \\ell_{d}<br>\n\\end{equation}</p>\n<p>where $\\boldsymbol \\ell_d$ is the objective for training of the detection network, and $\\boldsymbol \\ell_c$ is the objective for training of the correction network, which is also the final decision, and $\\lambda$ is the coefficient.</p>\n<h1>Reference</h1>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2005.07421\">https://arxiv.org/abs/2005.07421</a>, origin paper</li>\n</ul>\n","categories":["Natural Language Processing"],"tags":["Paper Reading","Spelling Error Correction"]},{"title":"LSTM Explained","url":"/posts/LSTM_Explained/","content":"<blockquote>\n<p>åœ¨æ—¶åºç›¸å…³æˆ–è€…æ–‡æœ¬ç›¸å…³çš„ä»»åŠ¡ä¸­ï¼Œäººä»¬å¸¸å¸¸ä¼šä½¿ç”¨<code>RNN-Based</code>çš„åºåˆ—æ¨¡å‹ï¼Œæœ¬æ–‡å°†ä¸»è¦ä»‹ç»<code>LSTM</code>æ¶‰åŠåˆ°çš„ä¸€äº›çŸ¥è¯†ç‚¹ã€‚ä¸»è¦å‚è€ƒå‡ ç¯‡ä¼˜ç§€çš„åšæ–‡ï¼Œé“¾æ¥å°†æ”¾åœ¨æœ€åçš„<a href=\"#jump\">Reference</a>ä¸­ï¼Œæ„Ÿå…´è¶£çš„å»ºè®®ç›´æ¥é˜…è¯»åŸæ–‡ğŸš˜ã€‚</p>\n</blockquote>\n<a id=\"more\"></a>\n<p>ä¸ºäº†é¿å…<code>Vanilla RNN</code>åœ¨è¾“å…¥é•¿æ–‡æœ¬åå¸¦æ¥çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†<code>LSTM unit</code>ï¼Œå³<code>long-short term memory unit</code>ï¼Œä½¿ç”¨å®ƒä»¬ä½œä¸ºæ•´ä¸ªåºåˆ—æ¨¡å‹ä¸­çš„æœ€å°å•å…ƒã€‚</p>\n<p>åœ¨<code>LSTM unit</code>ä¸­å…±å­˜åœ¨ä¸¤ç§<code>state</code>ï¼Œä¸€ç±»æ˜¯<code>hidden state</code>ï¼Œå¦ä¸€ç±»æ˜¯<code>cell state</code>ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†<code>é—¨æœºåˆ¶</code>ï¼Œåˆ†åˆ«ä¸º<code>Forget Gate</code>ã€<code>Update Gate/Input Gate</code>ä»¥åŠ<code>Output Gate</code>ã€‚åœ¨æ¯ä¸€ä¸ª<code>time step</code>æ—¶ï¼Œæ¨¡å‹é€šè¿‡<code>ä¸Šè¿°çš„é—¨æœºåˆ¶</code>å†³å®šåº”è¯¥å­˜æ”¾å“ªäº›ä¿¡æ¯ï¼ŒåŒæ—¶è¿‡æ»¤æ‰å“ªäº›ä¿¡æ¯ã€‚</p>\n<p>é¦–å…ˆï¼Œä¸ºäº†é˜æ˜<code>LSTM unit</code>çš„è®¡ç®—æœºåˆ¶ï¼Œå…ˆå¯¹å…¶å†…éƒ¨æ¶æ„è¿›è¡Œä¸€å®šçš„äº†è§£ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>\n<p><img src=\"https://miro.medium.com/max/700/0*exoKHMF9vYA3ZJvJ.png\" alt=\"LSTM unit\"><center style=\"font-size:14px;color:#C0C0C0\">Fig 1. LSTM unit, from <a href=\"https://miro.medium.com/max/700/0*exoKHMF9vYA3ZJvJ.png\">https://miro.medium.com/max/700/0*exoKHMF9vYA3ZJvJ.png</a></center></p>\n<p>é€šè¿‡<code>Fig 1</code>ï¼Œæˆ‘ä»¬å¯ä»¥é¢„è§ˆåˆ°<code>LSTM</code>ä¼ æ’­æ—¶æ•°æ®çš„å¤§è‡´æµé€šè¿‡ç¨‹ã€‚æ›´è¿‘ä¸€æ­¥çš„ï¼Œé€šè¿‡ä»¥ä¸‹åŠ¨å›¾å¯ä»¥ä¸€ç›®äº†ç„¶åœ°è§‚å¯Ÿåˆ°æ•°æ®æ˜¯æ€ä¹ˆåœ¨ä¸€ä¸ª<code>unit</code>ä¸­æµåŠ¨çš„ã€‚</p>\n<p><img src=\"https://miro.medium.com/proxy/1*goJVQs-p9kgLODFNyhl9zA.gif\" alt=\"Long Short Term Memory with its gates\"><center style=\"font-size:14px;color:#C0C0C0\">Fig 2. Data flow in LSTM unit, from <a href=\"https://miro.medium.com/proxy/1*goJVQs-p9kgLODFNyhl9zA.gif\">https://miro.medium.com/proxy/1*goJVQs-p9kgLODFNyhl9zA.gif</a></center></p>\n<p>å¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°ï¼Œåœ¨æ—¶é—´æ­¥<code>t</code>æ—¶ï¼Œ<code>unit</code>æ¥æ”¶æ¥è‡ªæ—¶é—´æ­¥<code>t-1</code>çš„$c_{t-1}$ä¸$h_{t-1}$ï¼Œç»è¿‡è‹¥å¹²æ¬¡çš„çº¿æ€§å˜åŒ–ä»¥åŠpoint-wiseæ“ä½œåï¼Œè¾“å‡º<code>c_t</code>ä¸<code>h_t</code>ã€‚ä¸‹é¢å°†å…·ä½“ä»‹ç»æ¯ä¸ªé—¨æœºåˆ¶çš„è¿è¡Œè¿‡ç¨‹ä»¥åŠä¸ªäººå¯¹äºè¯¥è®¾è®¡çš„ç†è§£ã€‚</p>\n<p>é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥å°†å†…éƒ¨è®¡ç®—ä¸­æ¶‰åŠåˆ°é—¨è®¡ç®—çš„éƒ¨åˆ†åˆ†ä¸ºä»¥ä¸‹å‡ ç§ã€‚</p>\n<p><img src=\"https://miro.medium.com/max/700/0*G474BVfgtu5ZE4ai\" alt=\"LSTM unit with gates\"><center style=\"font-size:14px;color:#C0C0C0\">Fig 3. Three types of gates in LSTM unit, from <a href=\"https://miro.medium.com/max/700/0*G474BVfgtu5ZE4ai\">https://miro.medium.com/max/700/0*G474BVfgtu5ZE4ai</a></center></p>\n<p>å…¶ä¸­ï¼š</p>\n<ul>\n<li>æœ€å·¦è¾¹çš„éƒ¨åˆ†ä¸º<code>Forget Gate</code>;</li>\n<li>ä¸­é—´çš„éƒ¨åˆ†ä¸º<code>Input Gateæˆ–Update Gate</code>;</li>\n<li>æœ€å³è¾¹çš„éƒ¨åˆ†ä¸º<code>Output Gate</code>.</li>\n</ul>\n<h3 id=\"Forget-Gate\">Forget Gate</h3>\n<blockquote>\n<p>å¯¹äºä¸Šä¸€ä¸ªæ—¶é—´æ­¥ä¼ é€’çš„<code>cell state</code>ä¿¡æ¯ï¼Œ<code>é—å¿˜é—¨</code>å†³å®šå“ªäº›ä¿¡æ¯éœ€è¦è¢«ç»§ç»­ä¿æŒï¼Œå“ªäº›ä¿¡æ¯åº”è¢«é—å¿˜ã€‚</p>\n</blockquote>\n<p>å‡è®¾å½“å‰æ‰€åœ¨çš„æ—¶é—´æ­¥ä¸º<code>t</code>ï¼Œè®°å½“å‰<code>forget gate</code>çš„è¾“å…¥ä¸º$X_t$ï¼Œ$c_{t-1}$ä»¥åŠ$h_{t-1}$ï¼Œè¾“å‡ºä¸º$C_{tmp}$ï¼Œæ ¹æ®ä¸Šé¢åŠ¨å›¾ä¸­çš„è®¡ç®—ï¼Œå…¶ç›¸åº”çš„è®¡ç®—ä¼ªä»£ç å¦‚ä¸‹ï¼š</p>\n<ol>\n<li>å°†$X_t$ä¸$h_{t-1}$è¿›è¡Œ<code>concatenation</code>ï¼Œå¾—åˆ°$[h_{t-1}, X_{t}]$ï¼›</li>\n<li>é€šè¿‡$W_{forget}$ä¸$b_{forget}$è¿›è¡Œçº¿æ€§è½¬æ¢ï¼Œå†é€šè¿‡<code>sigmoid</code>å°†è®¡ç®—ç»“æœè½¬æ¢åˆ°<code>[0, 1]</code>åŒºé—´ï¼›</li>\n<li>æœ€åï¼Œå°†ä¸Šè¿°è¾“å‡ºçš„<code>æ¦‚ç‡å‘é‡</code>ä¸$c_{t-1}$ä¸­ä¿å­˜çš„å‘é‡åš<code>point-wise</code>ä¹˜æ³•ã€‚</li>\n</ol>\n<p>ä¸‹é¢æ˜¯<code>Forget Gate</code>çš„è®¡ç®—å…¬å¼ï¼š</p>\n<ol>\n<li>$Conbine = Concatenation(h_{t-1}, X_t)$</li>\n<li>$Z_f = Sigmoid(W_{forget} Conbine + b_{forget})$</li>\n<li>$c_{tmp} = c_{t-1} * Z_f$</li>\n</ol>\n<!-- \\begin{equation*}\n\\tag{1}\n\\label{a}\n\\begin{cases}\n    Conbine_{forget} = Concatenation(h_{t-1}, X_t) \\\\\\\\\n    O = Sigmoid(W_{forget} Conbine_{forget} + b_{forget}), \\\\\\\\\n    C_{t1} = C_{t-1} * O\n\\end{cases}\n\\end{equation*} -->\n<p>é€šè¿‡<code>Forget Gate</code>çš„ä¸€ç³»åˆ—æ“ä½œï¼Œä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„$c_{t-1}$ç¬¬ä¸€æ¬¡å¾—åˆ°äº†æ›´æ–°ï¼Œå¾—åˆ°äº†$c_{tmp}$ã€‚æˆ‘ä»¬ç”¨å®ƒè¡¨ç¤ºåœ¨æ—¶é—´æ­¥<code>t</code>è¾“å‡ºæœ€ç»ˆ<code>cell state</code>å‰çš„ä¸­é—´å€¼ã€‚</p>\n<p>æˆ‘è®¤ä¸ºé—å¿˜é—¨è¿ä½œçš„æœºåˆ¶æ˜¯ï¼šå°†ä¹‹å‰ä¿å­˜çš„ä¿¡æ¯ï¼ˆå­˜æ”¾åœ¨$h_{t-1}$ï¼‰ä¸å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ä¿¡æ¯ï¼ˆ$X_t$ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œè¿›è€Œå»åˆ¤æ–­æ˜¯å¦å¯¹$c_{t-1}$ä¸­çš„æŸäº›ä¿¡æ¯è¿›è¡Œé—å¿˜ã€‚å½“<code>sigmoid</code>è¾“å‡ºçš„å‘é‡ä¸­æŸä¸ªä½ç½®è¾“å‡ºçš„æ¦‚ç‡å€¼æ›´åå‘äº0ï¼Œåˆ™è¯´æ˜$c_{t-1}$å¯¹åº”ä½ç½®ä¸Šçš„ä¿¡æ¯åº”è¯¥è¢«é—å¿˜ï¼›è€Œå½“è¯¥ä½ç½®çš„æ¦‚ç‡å€¼æ›´åå‘1æ—¶ï¼Œåˆ™$c_{t-1}$å¯¹åº”ä½ç½®ä¸Šçš„ä¿¡æ¯æ›´åº”è¯¥ä¿ç•™ã€‚</p>\n<h3 id=\"Update-Gate-Input-Gate\">Update Gate/Input Gate</h3>\n<blockquote>\n<p><code>æ›´æ–°é—¨</code>éœ€è¦è€ƒè™‘å°†å“ªäº›æ–°ä¿¡æ¯ä¿å­˜æˆ–è€…è´´åŠ åˆ°<code>cell state</code>ä¸­ï¼Œå¹¶ä¸”è¾“å‡ºåˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ã€‚</p>\n</blockquote>\n<p>åœ¨<code>Update Gate</code>ä¸­ï¼Œ<code>unit</code>å†³å®š<code>ä»€ä¹ˆä¿¡æ¯</code>è¦è¢«æ›´æ–°åˆ°$c_{tmp}$ä¸­ã€‚è¿™ç»„é—¨è¿ç®—çš„è¾“å…¥åŒ…æ‹¬ï¼š$h_{t-1}, X_t, c_{tmp}$ï¼Œè¾“å‡ºä¸º$c_{t}$ï¼š</p>\n<ol>\n<li>ç¬¬ä¸€æ­¥è¿˜æ˜¯åŒ<code>Forget Gate</code>ä¸­å½¢å¼çš„ä¸€æ ·ï¼Œå…ˆå°†$X_t$ä¸$h_{t-1}$è¿›è¡Œ<code>concatenation</code>ï¼Œå¾—åˆ°$[h_{t-1}, X_{t}]$ï¼›</li>\n<li>æ¥ä¸‹æ¥ï¼Œåˆ†åˆ«è¿›è¡Œä¸¤æ¬¡çº¿æ€§å˜æ¢ï¼Œä½†æ˜¯å„è‡ª<code>activation function</code>ä¸åŒï¼Œä¸€ä¸ªä¸º<code>sigmod</code>ï¼Œè€Œå¦ä¸€ä¸ªåˆ™ä¸º<code>tanh</code>ï¼›</li>\n<li>æœ€åï¼Œå°†ä¸¤ä¸ªç»“æœè¿›è¡Œ<code>point-wise</code>çš„ä¹˜æ³•ï¼Œå¾—åˆ°<code>Update Gate</code>çš„è¾“å‡ºã€‚</li>\n</ol>\n<p>åŒæ ·çš„ï¼Œä¸‹é¢æ˜¯<code>Update Gate</code>çš„è®¡ç®—å…¬å¼ï¼š</p>\n<ol>\n<li>$Conbine = Concatenation(h_{t-1}, X_t)$</li>\n<li>$Z_u = Sigmoid(W_{update1} Conbine + b_{update1})$</li>\n<li>$O_u = Tanh(W_{update2} Conbine + b_{update2})$</li>\n<li>$c_{t} = c_{tmp} + Z_u * O_u$</li>\n</ol>\n<p><code>æ›´æ–°é—¨</code>ä¸­å…±å­˜åœ¨ä¸¤ç»„<code>çº¿æ€§è½¬åŒ–</code>çš„å‚æ•°ï¼š$(W_{update1}, b_{update1})$ä»¥åŠ$(W_{update2}, b_{update2})$ï¼Œå„è‡ªçš„ç»“æœè½¬åŒ–ä¹‹åç›¸ä¹˜ï¼Œå†åŠ åˆ°$c_{tmp}$ä¸Šï¼Œæœ€åå¾—åˆ°å½“å‰æ—¶é—´æ­¥å°†è¦è¾“å‡ºçš„<code>cell state</code>ã€‚</p>\n<h3 id=\"Output-Gate\">Output Gate</h3>\n<blockquote>\n<p><code>è¾“å‡ºé—¨</code>çš„ä½œç”¨åœ¨äºç»“åˆå„ç§ä¿¡æ¯ï¼Œåœ¨å½“å‰çš„æ—¶é—´æ­¥åšå‡ºå†³ç­–ï¼ŒåŒæ—¶ä¸ºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—æä¾›ä¿¡æ¯ã€‚</p>\n</blockquote>\n<p>é™¤å»ä¸Šè¿°çš„ä¸¤ç»„ï¼Œ<code>unit</code>ä¸­è¿˜å‰©ä¸‹ä¸€ç»„é—¨è¿ç®—ï¼Œå³<code>è¾“å‡ºé—¨</code>ã€‚å®ƒçš„è¾“å…¥åŒ…æ‹¬$h_{t-1}, X_t, c_{t}$ï¼Œæœ€åçš„è¾“å‡ºä¸º$h_t$:</p>\n<ol>\n<li>ç¬¬ä¸€æ­¥ä¾æ—§æ˜¯æ‹¼æ¥$X_t$ä¸$h_{t-1}$ï¼Œå¾—åˆ°$[h_{t-1}, X_{t}]$ï¼›</li>\n<li>é€šè¿‡$W_{output}$ä¸$b_{output}$è¿›è¡Œçº¿æ€§è½¬æ¢ï¼Œå†é€šè¿‡<code>sigmoid</code>å°†è®¡ç®—ç»“æœè½¬æ¢åˆ°<code>[0, 1]</code>åŒºé—´ï¼›</li>\n<li>å°†$c_t$é€šè¿‡<code>tanh</code>è¿ç®—ï¼›</li>\n<li>æœ€åå°†2ã€3æ­¥çš„è¾“å‡ºåš<code>point-wise</code>çš„ä¹˜æ³•ï¼Œå¾—åˆ°<code>Output Gate</code>çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯$h_t$ã€‚</li>\n</ol>\n<p>åŒæ ·ï¼Œä¸‹é¢æ˜¯<code>Output Gate</code>çš„è®¡ç®—å…¬å¼ï¼š</p>\n<ol>\n<li>$Conbine = Concatenation(h_{t-1}, X_t)$</li>\n<li>$Z_o = Sigmoid(W_{output} Conbine + b_{output})$</li>\n<li>$O_o = Tanh(c_t)$</li>\n<li>$h_t = Z_o * O_o$</li>\n</ol>\n<h2 id=\"æ€»ç»“\">æ€»ç»“</h2>\n<p>åˆ°æ­¤ï¼Œä¸€ä¸ª<code>LSTM unit</code>ä¸­çš„è®¡ç®—æµç¨‹å·²ç»èµ°å®Œï¼Œä¸‹é¢æˆ‘æƒ³è°ˆä¸€ä¸‹ä¸ªäººå¯¹äºå…¶ä¸­å‡ ç‚¹çš„ç†è§£ã€‚é¦–å…ˆæ˜¯å‡ºç°äº†ä¸¤ç§æ¿€æ´»å‡½æ•°<code>sigmoid</code>ä»¥åŠ<code>tanh</code>ã€‚å…¶ä¸­ï¼Œ<code>sigmoid</code>å…±å‡ºç°äº†3æ¬¡ï¼Œåˆ†åˆ«åœ¨ä¸‰ä¸ªé—¨ä¸­éƒ½å‡ºç°ï¼›<code>tanh</code>åˆ™æ˜¯å‡ºç°åœ¨<code>Update Gate</code>ä»¥åŠ<code>Output Gate</code>ä¸­ã€‚</p>\n<p><code>sigmoid</code>èƒ½å°†æ•°å€¼ç¼©æ”¾è‡³[0, 1]åŒºé—´ï¼Œå®ƒåœ¨<code>LSTM unit</code>ä¸­çš„ä½¿ç”¨éƒ½æ˜¯ä¸å…¶ä»–å‘é‡åš<code>point-wise</code>çš„ä¹˜æ³•ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºä½¿ç”¨å®ƒä¸»è¦æ˜¯ä¸ºäº†ä½¿<code>unit</code>åœ¨å¤„ç†æ—¶èƒ½å¤Ÿå°†ä¿¡æ¯è¿›è¡Œä¿å­˜æˆ–è€…é—å¿˜ã€‚</p>\n<p><code>tanh</code>èƒ½å°†æ•°å€¼ç¼©æ”¾è‡³[-1, 1]åŒºé—´ï¼Œå®ƒåœ¨<code>LSTM unit</code>ä¸­è¢«ä½¿ç”¨æ—¶åŸºæœ¬ä¸Šæ˜¯ä¸ºäº†å°†<code>ä¿¡æ¯</code>è½¬åŒ–ä¸º<code>æ•°å€¼</code>ï¼Œåœ¨<code>Update Gate</code>ä¸­ï¼Œè¯¥æ•°å€¼ä¼šè¢«è®°å½•åˆ°<code>cell state</code>ä¸­ï¼Œåœ¨<code>Output Gate</code>ä¸­ï¼Œè¯¥æ•°å€¼ä¼šè¢«è½¬åŒ–ä¸ºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„<code>hidden state</code>ã€‚</p>\n<h2 id=\"span-id-jump-Reference-span\"><span id=\"jump\">Reference</span></h2>\n<ul>\n<li>\n<p><a href=\"https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\">Illustrated Guide to Recurrent Neural Networks</a></p>\n</li>\n<li>\n<p><a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a></p>\n</li>\n<li>\n<p><a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">Illustrated Guide to LSTMâ€™s and GRUâ€™s: A step by step explanation</a></p>\n</li>\n<li>\n<p><a href=\"https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#recurrent-neural-networks\">Written Memories: Understanding, Deriving and Extending the LSTM</a></p>\n</li>\n<li>\n<p><a href=\"https://purnasaigudikandula.medium.com/recurrent-neural-networks-and-lstm-explained-7f51c7f6bbb9\">Recurrent Neural Networks and LSTM explained</a></p>\n</li>\n</ul>\n","categories":["Natural Language Processing"],"tags":["Sequence Model"]},{"title":"BERT Explained","url":"/posts/BERT_Explained/","content":"<blockquote>\n<p>BERT, which means $Bidirectional \\ Encoder \\ Representations \\ from \\ Transformers$, is one kind of SOAT models in natural language preprocessing over multiple tasks. In this arctile, I want to note my opnion of this model architecture and its training method.</p>\n</blockquote>\n<a id=\"more\"></a>\n<h2 id=\"Beginning\">Beginning</h2>\n<p>BERT is proposed in 2018, from the paper <a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>. We can know its model architecture derived from Transformer, which is also proposed by Google in 2017 from the paper <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>. Every time I mention these two amazing papers, I can hardly conceal my excitement. They provide creative ideas for computers to understand natural language. I even think all practitioners related to NLP should have basic knowledge of $BERT$.</p>\n<p>To explain $BERT$ clearly, I plan to introduce Machine Translation task, and its most common model architecture Sequence to Sequence firstly. Then, Iâ€™ll introduce the Transformer, which is one kind of <code>Seq2Seq</code> models but involving <code>Attention Mechanism</code> and the performance has been greatly imporved. Finally, weâ€™ll focus on BERT.</p>\n<p>This article is my summary for related work and hope itâ€™s helpful. Simultaneously, there are so many awesome articles and I list them at the end part <code>Reference</code>. Thanks for these authorsâ€™ excellent work to make all the core concepts clear.</p>\n<h2 id=\"Sequence-to-Sequence\">Sequence to Sequence</h2>\n<p>$Seq2Seq$æœ€å¸¸è§çš„åº”ç”¨å¸¸è§ä¾¿æ˜¯<code>æœºå™¨ç¿»è¯‘</code>ï¼Œåœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼Œè¿™ä¸ªé¢†åŸŸçš„ç ”ç©¶æœ‰é•¿è¶³çš„çªç ´ï¼Œå¾ˆå¤šå•†ä¸šè½¯ä»¶æ¯”å¦‚è°·æ­Œç¿»è¯‘ã€æœ‰é“ç¿»è¯‘ç­‰ç­‰è¾¾åˆ°äº†éå¸¸å¥½çš„ç¿»è¯‘æ•ˆæœã€‚ç®€å•æ¥è¯´ï¼Œæœºå™¨ç¿»è¯‘ä»»åŠ¡å®ç°çš„æ˜¯ä¸åŒå›½å®¶æˆ–è€…åœ°åŒºé—´è¯­è¨€çš„è½¬æ¢ï¼Œèƒ½å¸®åŠ©äººä»¬æ›´åŠ ç•…é€šçš„æ²Ÿé€šã€‚</p>\n<p>é™¤äº†ä¸Šè¿°æåˆ°çš„æœºå™¨ç¿»è¯‘ï¼Œ$Seq2Seq$ç†è®ºä¸Šæ¥è¯´é€‚ç”¨äºä»»ä½•åºåˆ—æ–‡æœ¬é—´çš„è½¬æ¢ï¼Œæ¯”å¦‚ç¼–ç¨‹è¯­è¨€é—´çš„è½¬æ¢ï¼Œä»<code>C++</code>è½¬æ¢åˆ°<code>Golang</code>ã€‚ç”šè‡³ï¼Œé‚£äº›æœ¬å°±ä¸å­˜åœ¨çš„è¯­è¨€ï¼Œä½†æ˜¯åªè¦æœ‰ç€ä¸€å®šé‡çš„è®­ç»ƒæ•°æ®ï¼Œ$Seq2Seq$æ€»èƒ½ä¸ºæˆ‘ä»¬å‘ç°æºè¯­è¨€ä¸ç›®æ ‡è¯­è¨€çš„æ˜ å°„å…³ç³»ï¼Œå¹¶ä¸”å¸®åŠ©æˆ‘ä»¬å»ºç«‹è¿™ç§å…³ç³»ã€‚</p>\n<p>é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¿˜è®°$Seq2Seq$æ¶æ„ï¼Œæƒ³ä¸€æƒ³å¦‚ä½•æè¿°æœºå™¨ç¿»è¯‘è¿™ä¸ªåœºæ™¯ã€‚å½¼æ—¶å­˜åœ¨è¾“å…¥æ–‡æœ¬$\\mathbf{x}={x_1, x_2, \\ldots, x_n}$ï¼Œé€šè¿‡ç¿»è¯‘ä¹‹åï¼Œå°†ä¼šå¾—åˆ°å¦å¤–ä¸€ç»„åºåˆ—æ–‡æœ¬$\\mathbf{y}={y_1, y_2, \\ldots, y_m}$ã€‚å¯ä»¥ä½¿ç”¨æ¦‚ç‡ $p(\\mathbf{y} \\mid \\mathbf{x})$ è¡¨ç¤ºï¼š<br>\n$$<br>\n\\mathbf{y}^{*}=\\arg \\max_{\\mathbf{y}} p(\\mathbf{y} \\mid \\mathbf{x})<br>\n$$<br>\nç†è®ºä¸Š$\\mathbf{y}$çš„ç©ºé—´æ˜¯æ— ç©·å¤§çš„ï¼Œå³å¯èƒ½å‡ºç°å¾ˆå¤šä¸ªç¿»è¯‘ç»“æœï¼Œä½†æœ€åéœ€è¦çš„æ˜¯é‚£ä¸ªæ¦‚ç‡æœ€å¤§çš„è¾“å‡ºã€‚</p>\n<p>æ¥ç€ï¼Œå°†ä¸Šè¿°çš„å¼å­è¿›è¡Œæ”¹å†™ï¼ŒåŠ å…¥æ¨¡å‹çš„å‚æ•°$\\theta$ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š<br>\n$$<br>\n\\mathbf{y}^{*}=\\arg \\max_{\\mathbf{y}} p(\\mathbf{y} \\mid \\mathbf{x}, \\theta)<br>\n$$<br>\nç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å®šä¹‰äº†ä¸€ä¸ªå…³äº<code>æœºå™¨ç¿»è¯‘åœºæ™¯</code>çš„æ¦‚ç‡æ¨¡å‹ã€‚å…¶å®ï¼Œå’Œå®ƒç›¸ä¼¼çš„æ¦‚ç‡æ¨¡å‹è¿˜æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚å®ä½“è¯†åˆ«ï¼ŒåŒæ ·å¦‚ä¸Šè¿°å®šä¹‰ï¼Œåªä¸è¿‡è¾“å…¥$\\mathbf{x}$ä¸è¾“å‡º$\\mathbf{y}$çš„é•¿åº¦æ˜¯ä¸€æ ·çš„ã€‚ä¹‹åè¦è€ƒè™‘çš„é—®é¢˜ä¸»è¦å›´ç»•ä¸‰ä¸ªæ–¹é¢ï¼š</p>\n<ol>\n<li>å¦‚ä½•å»ºæ¨¡ï¼Œå³å‚æ•°$\\theta$é•¿ä»€ä¹ˆæ ·å­ï¼Ÿ</li>\n<li>å¦‚ä½•å»è®­ç»ƒå‚æ•°ï¼Ÿ</li>\n<li>å¦‚ä½•æ¨ç†ï¼Ÿ</li>\n</ol>\n<p>å®é™…ä¸Šï¼Œå¯¹äºå¤§å¤šæ•°æ¶‰åŠåˆ°æ¦‚ç‡æ¨¡å‹å»è§£å†³ä¸šåŠ¡é—®é¢˜çš„ï¼Œæ— è®ºæ˜¯åŸºäºè”åˆæ¦‚ç‡åˆ†å¸ƒçš„æ¯”å¦‚<code>HMM</code>æˆ–æ˜¯åŸºäºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„æ¯”å¦‚<code>CRF</code>æˆ–æ˜¯ä¹‹åè¦ä»‹ç»çš„<code>Seq2Seq</code>ï¼Œæœ€åæ±‚è§£çš„æ—¶å€™å°±æ˜¯è¿™ä¸‰æ¿æ–§ã€‚</p>\n<h1>Reference</h1>\n<ul>\n<li><a href=\"https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#main_content\">The best and intuitive blog I have ever read about Seq2Seq, Attention, Transformers and etc. Some parts of my article can be regarded as the chinese version of this blog.</a></li>\n<li><a href=\"http://jalammar.github.io/illustrated-transformer/\">Illustrated Transformer</a></li>\n<li><a href=\"http://zh.gluon.ai/chapter_natural-language-processing/seq2seq.html\">The best practical and theoretical Deep Learning Book, Dive into DL</a></li>\n</ul>\n","categories":["Natural Language Processing"],"tags":["Sequence Model","Transformers","BERT Family"]},{"title":"LoRA Explained","url":"/posts/LoRA_Explained/","content":"<blockquote>\n<p>è¿‘äº›æ—¶é—´ï¼Œå¤§æ¨¡å‹å¦‚é›¨åæ˜¥ç¬‹èˆ¬ï¼Œçªçš„ä¸€ä¸‹ï¼Œè¿›å…¥å…¬ä¼—è§†é‡ï¼Œè¯¸å¦‚è¯­è¨€é¢†åŸŸçš„ChatGPTï¼Œæˆ–æ˜¯å›¾åƒé¢†åŸŸçš„Stable Diffusionã€‚å®ƒä»¬åœ¨å„è‡ªé¢†åŸŸä¸Šå¸¦ç»™ç”¨æˆ·ä¸ä¿—çš„ä½¿ç”¨ä½“éªŒã€‚åœ¨ç®—æ³•åº”ç”¨å¼€å‘çš„è§’åº¦ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒèƒ½ä¸èƒ½åœ¨ç‰¹å®šçš„ç®—æ³•ç¯å¢ƒä¸­ä½¿ç”¨ä¸Šè¿™äº›å…ˆè¿›çš„å¤§æ¨¡å‹ï¼Œè€Œåºå¤§çš„æ¨¡å‹å‚æ•°é‡ä¸ºè¿™ä¸ªé—®é¢˜è’™ä¸Šä¸€äº›ä¸ç¡®å®šæ€§ã€‚æœ¬æ–‡è¦ä»‹ç»çš„<code>LoRA</code>æ— ç–‘æ˜¯ä¸ºå¤§æ¨¡å‹çš„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„å¯èƒ½ã€‚</p>\n</blockquote>\n<a id=\"more\"></a>\n<p>è¿‘äº›æ—¶é—´ï¼Œå¤§æ¨¡å‹å¦‚é›¨åæ˜¥ç¬‹èˆ¬ï¼Œçªçš„ä¸€ä¸‹ï¼Œè¿›å…¥å…¬ä¼—è§†é‡ï¼Œè¯¸å¦‚è¯­è¨€é¢†åŸŸçš„<code>ChatGPT</code>ï¼Œæˆ–æ˜¯å›¾åƒé¢†åŸŸçš„<code>Stable Diffusion</code>ã€‚å®ƒä»¬åœ¨å„è‡ªé¢†åŸŸä¸Šå¸¦ç»™ç”¨æˆ·ä¸ä¿—çš„ä½¿ç”¨ä½“éªŒã€‚åŒæ—¶ï¼Œä¹Ÿä¸ç¦ä»¤äººæ€è€ƒï¼Œ<code>AIGC</code>åˆ°åº•èƒ½å†å¾€å‰è¿›åŒ–åˆ°ä½•ç§ç¨‹åº¦ï¼Ÿ</p>\n<p>åœ¨<code>ChatGPT</code>å¦‚æ—¥ä¸­å¤©ï¼Œé¼æ²¸åˆ°åœ¨é£Ÿå ‚æ’é˜Ÿéƒ½èƒ½å¬åˆ°å…¶ä»–åŒäº‹ä¹æ­¤ä¸ç–²åœ°è®¨è®ºæ—¶ï¼Œæˆ‘å¯¹å®ƒçš„â€œè½åœ°â€å¹¶ä¸æŠ±æœ‰æœŸå¾…ã€‚å› ä¸ºåœ¨ç®—æ³•åº”ç”¨å¼€å‘çš„è§’åº¦ï¼Œæˆ‘ä»¬æ›´å…³å¿ƒèƒ½ä¸èƒ½åœ¨ç‰¹å®šçš„ç®—æ³•ç¯å¢ƒä¸­ä½¿ç”¨ä¸Šè¿™äº›å…ˆè¿›çš„å¤§æ¨¡å‹ï¼Œè€Œåºå¤§çš„æ¨¡å‹å‚æ•°é‡ä¸ºè¿™ä¸ªé—®é¢˜è’™ä¸Šä¸€äº›ä¸ç¡®å®šæ€§ã€‚</p>\n<h2 id=\"Background\">Background</h2>\n<h4 id=\"LLM-Parameters\">LLM Parameters</h4>\n<table>\n<thead>\n<tr>\n<th><strong>å…¬å¸</strong></th>\n<th><strong>æ¨¡å‹</strong></th>\n<th><strong>å‚æ•°é‡ï¼ˆBilionï¼‰</strong></th>\n<th><strong>è®¡ç®—èµ„æº</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>OpenAI</td>\n<td>GPT-3</td>\n<td>175</td>\n<td>30000+ A100</td>\n</tr>\n<tr>\n<td>Google</td>\n<td>PaLM-E</td>\n<td>562</td>\n<td>/</td>\n</tr>\n<tr>\n<td>Meta</td>\n<td>LLaMA</td>\n<td>7/13/33/65</td>\n<td>2048 A100 for 5 months</td>\n</tr>\n</tbody>\n</table>\n<p><font size=2><strong>æ³¨ï¼š</strong><code>bert-base</code>çš„å‚æ•°é‡æ˜¯<code>110 milion</code></font></p>\n<p>åŸºäºæ‹¥æœ‰å¦‚æ­¤åºå¤§å‚æ•°é‡çš„å¤§æ¨¡å‹ï¼Œåœ¨è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„<code>fine-tuning</code>æ—¶ï¼Œæ›´æ–°<code>LLM</code>çš„å…¨éƒ¨å‚æ•°éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</p>\n<h4 id=\"Whatâ€™s-LoRA\">Whatâ€™s LoRA</h4>\n<p><a href=\"https://arxiv.org/abs/2106.09685\">LoRA</a>ï¼Œå³<code>low-rank adapation</code>çš„ç¼©å†™ï¼Œå®ƒæ˜¯ä¸€ç§åº”ç”¨åœ¨<code>LLM fine-tuning</code>é˜¶æ®µçš„è®­ç»ƒæ–¹å¼ã€‚å®ƒèƒ½å¸®åŠ©ä»¥è¾ƒå°‘çš„è®¡ç®—èµ„æºå’Œå¼€é”€è¿›è¡Œ<code>LLM fine-tuning</code>ï¼Œæ¯”è¾ƒçŸ¥åçš„é¡¹ç›®æœ‰ï¼š</p>\n<ol>\n<li><a href=\"https://github.com/tloen/alpaca-lora\">Alpaca-LoRA</a></li>\n<li><a href=\"https://github.com/cloneofsimo/lora\">Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning</a></li>\n</ol>\n<p>åŸºäº<code>LoRA fine-tuning</code>çš„æ¨¡å‹æ€§èƒ½æ²¡æœ‰è¿‡å¤šé™ä½ã€‚åœ¨è®ºæ–‡çš„å®éªŒéƒ¨åˆ†ï¼Œç”šè‡³è¿˜æœ‰ä¸€äº›ä»»åŠ¡åè¶…äº†<code>fully fine-tuned model</code>ã€‚</p>\n<h2 id=\"Related-Works\">Related Works</h2>\n<h4 id=\"Adding-adapater-layers\">Adding adapater layers</h4>\n<p>è¯¥ç±»æ–¹æ³•çš„ä¸»è¦æ€æƒ³å°±æ˜¯åœ¨å¤§æ¨¡å‹ä¸­æ–°å¢ä¸€äº›<code>adapter layers</code>ï¼Œåœ¨<code>fine-tuning</code>è¿‡ç¨‹ä¸­ï¼Œä»…æ›´æ–°è¿™äº›æ–°å¢çš„å‚æ•°ï¼Œé¿å…å¯¹å¤§æ¨¡å‹æ•´ä½“å‚æ•°çš„æ›´æ–°ï¼Œä»¥è¾¾åˆ°é™ä½è®¡ç®—å¼€é”€çš„ç›®çš„ã€‚ä»¥ä¸‹ä¸ºéƒ¨åˆ†å·¥ä½œï¼š</p>\n<ul>\n<li>2017ï¼Œ<a href=\"http://arxiv.org/abs/1705.08045\">Learning multiple visual domains with residual adapters.</a></li>\n<li>2019ï¼Œ<a href=\"http://arxiv.org/abs/1902\">Parameter-Efficient Transfer Learning for NLP.</a></li>\n<li>2020ï¼Œ<a href=\"https://aclanthology.org/2020.findings-emnlp.41\">Exploring versatile generative language model via parameter-efficient transfer learning.</a></li>\n</ul>\n<p>ä¸¥æ ¼æ¥è¯´ï¼Œ<code>LoRA</code>ä¹Ÿå±äºè¿™ç§æ–¹å¼ï¼Œä½†æ˜¯ç›¸æ¯”äºä¸Šè¿°å·¥ä½œï¼Œå®ƒåœ¨æ¨ç†æ—¶çš„é€Ÿåº¦å¹¶ä¸ä¼šå› ä¸ºæ–°å¢çš„å‚æ•°è€Œé™ä½ï¼Œåç»­ä¼šè¯¦ç»†ä»‹ç»å®ƒçš„è®¡ç®—æ–¹å¼ã€‚</p>\n<h4 id=\"Optimizing-the-input-word-embedding\">Optimizing the input word embedding</h4>\n<p>æ¯”è¾ƒæ–°é¢–çš„æ–¹æ³•ï¼Œ<code>Prefix-Tuning</code>æ—¨åœ¨<code>Embedding Layer</code>å¢åŠ é¢å¤–å‚æ•°ï¼Œå†»ç»“å‰©ä½™ç½‘ç»œå‚æ•°ï¼Œä»¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒã€‚</p>\n<ul>\n<li>2021ï¼Œ<a href=\"https://arxiv.org/pdf/2101.00190.pdf\">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></li>\n</ul>\n<h2 id=\"LoRA-Method\">LoRA Method</h2>\n<h4 id=\"Intrinsic-Dimension\">Intrinsic Dimension</h4>\n<p><img src=\"/images/LoRA/swiss_roll_data.png\" alt=\"Swiss roll data\"><center style=\"font-size:14px;color:#C0C0C0\">Swiss roll data curves, from <a href=\"https://twitter.com/lightonio/status/1240687522608373760\">https://twitter.com/lightonio/status/1240687522608373760</a></center></p>\n<p>From <a href=\"https://en.wikipedia.org/wiki/Intrinsic_dimension\">Wikipidia</a></p>\n<blockquote>\n<p>The intrinsic dimension for a data set can be thought of as the number of variables needed in a minimal representation of the data.</p>\n</blockquote>\n<p><a name=\"jaT4G\"></a></p>\n<h4 id=\"Fine-tune-LLM\">Fine-tune LLM</h4>\n<p>æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œæœ‰ç›‘ç£ç¥ç»ç½‘ç»œçš„è®­ç»ƒèŒƒå¼å¤§å¤šåŸºäº<strong>æ¢¯åº¦ä¸‹é™</strong>ï¼Œå³ä¸€è½®<code>batch data</code>è¿‡åï¼Œé€šè¿‡æœ¬è½®æ•°æ®è®¡ç®—<code>loss</code>æ›´æ–°ç½‘ç»œå‚æ•°$W$ã€‚å‡å®šå½“å‰è½®ä¸ºç¬¬$t$ï¼Œå³ï¼š</p>\n<p>$$<br>\n\\begin{equation}<br>\nW_{t+1} = W_t - lr * \\Delta{W_t}<br>\n\\end{equation}<br>\n$$<br>\nå¯¹äºæ¨¡å‹çš„è®­ç»ƒï¼Œå…¶æœ¬è´¨æ˜¯å‚æ•°$W$çš„ä¸æ–­æ›´æ–°ï¼Œè®°åˆå§‹å‚æ•°ä¸º$W_0$ï¼Œè®­ç»ƒç»“æŸå¾—åˆ°çš„å‚æ•°ä¸º$W_T$ã€‚å¯¹äº<code>LLM</code>æ¥è¯´ï¼Œ$W_0$ä»£è¡¨ä½œä¸º<code>Pretrained-model</code>çš„å‚æ•°ï¼Œé€šè¿‡å¤šè½®çš„è®­ç»ƒï¼Œç»å†å¤šä¸ª$\\Delta{W}$çš„æ›´æ–°åå¾—åˆ°$W_T$ã€‚åœ¨æ›´æ–°çš„è¿‡ç¨‹ä¸­ï¼Œæœ‰ï¼š</p>\n<p>$$<br>\n\\begin{equation}<br>\n\\begin{gathered}<br>\nW_1 = W_0 - lr * \\Delta{W_0} \\\\<br>\nW_2 = W_1 - lr * \\Delta{W_1} \\\\<br>\nW_3 = W_2 - lr * \\Delta{W_2} \\\\<br>\n\\ldots \\\\<br>\nW_T = W_{T-1} - lr * \\Delta{W_{T-1}}<br>\n\\end{gathered}<br>\n\\iff<br>\nW_T = W_0 - lr * (\\Delta{W_0} + \\Delta{W_1} + \\cdots + \\Delta{W_{T-1}})<br>\n\\end{equation}<br>\n$$</p>\n<p>ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œå¯¹æ¨¡å‹<code>fine-tuning</code>çš„è¿‡ç¨‹å°±åƒæ˜¯å­¦ä¹ ä¸€ä¸ªé€‚åº”<strong>ç‰¹å®šä»»åŠ¡</strong>çš„$\\Delta{W}$ï¼Œç»“åˆ$W_0$åŠ$\\Delta{W}$è¿›è¡Œæ¨ç†ï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚å› æ­¤ï¼Œè‹¥å°†$\\Delta{W}$ä½œä¸ºå¯è®­ç»ƒçš„å‚æ•°ï¼Œ<code>fine-tuning LLM</code>å³è½¬åŒ–ä¸ºå¯¹$\\Delta{W}$çš„æ‹Ÿåˆã€‚</p>\n<p><img src=\"/images/LoRA/before.png\" alt=\"å›¾1\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">å›¾1</center>\n<h4 id=\"Introduce-LoRA\">Introduce LoRA</h4>\n<p>æœ‰<a href=\"https://arxiv.org/abs/2012.13255\">è®ºæ–‡</a>åœ¨å®éªŒå¯¹æ¯”çš„è¿‡ç¨‹ä¸­ï¼Œå‘ç°<code>LLM</code>çš„å‚æ•°æœ‰ç€è¾ƒä½çš„$\\text{Intrinsic Demension}$ï¼Œå—æ­¤å¯å‘ï¼Œ<code>LoRA</code>çš„ä½œè€…å‡å®š$\\Delta{W}$ä¹Ÿå­˜åœ¨è¿™ç§ç‰¹æ€§ã€‚</p>\n<p>From <a href=\"https://arxiv.org/abs/2106.09685\">LoRA Paper</a></p>\n<blockquote>\n<p>Inspired by this, we hypothesize the updates to the weights also have a low â€œintrinsic rankâ€ during adapation.</p>\n</blockquote>\n<p>è‹¥$\\Delta{W}$å­˜åœ¨è¾ƒä½çš„$\\text{Intrinsic Rank}$ï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œ<strong>çŸ©é˜µåˆ†è§£</strong>$\\left(\\text{Matrix Factorization}\\right)$ï¼Œå³ï¼š<br>\n$$<br>\n\\begin{equation}<br>\n\\Delta{W} = BA<br>\n\\end{equation}<br>\n$$</p>\n<p>$\\Delta{W} \\in \\mathbb{R^{d*k}}, B \\in \\mathbb{R^{d*r}}, A \\in \\mathbb{R^{r*k}}, r \\ll \\min(d, k)$ï¼Œä½¿ç”¨$(3)$å¼è¡¨ç¤º$\\Delta{W}$ä¹‹åï¼Œå‚ä¸å­¦ä¹ çš„å‚æ•°é‡å¾—å€’ç¼©å‡ï¼Œç”±$O(d * k)$ç¼©å‡è‡³$O((d + k) * r)$ã€‚</p>\n<p><img src=\"/images/LoRA/after.png\" alt=\"å›¾2\"></p>\n<center style=\"font-size:14px;color:#C0C0C0\">å›¾2</center>\n<h2 id=\"Practice\">Practice</h2>\n<p><code>LoRA</code>çš„æƒ³æ³•çœ‹èµ·æ¥ååˆ†ç®€å•ï¼Œç›®å‰å¼€æºç¤¾åŒºæœ‰ä¸¤æ–¹å®ç°å…¶å·¥ç¨‹ä»£ç ã€‚</p>\n<ul>\n<li><a href=\"https://github.com/microsoft/LoRA\">è®ºæ–‡ä½œè€…</a></li>\n<li><a href=\"https://github.com/huggingface/peft\">hugging face PEFT</a></li>\n</ul>\n<p>åè€…ä¸»è¦å¯¹åœ¨<code>PyTorch FSDP</code>çš„è®­ç»ƒæ¨¡å¼ä¸Šè¿›è¡Œè°ƒæ•´ï¼Œä½†åœ¨ä½¿ç”¨å½¢å¼ä¸Šæ²¡æœ‰åŒºåˆ«ï¼Œä»¥ä¸‹åŸºäº<strong>è®ºæ–‡ä½œè€…</strong>çš„ç‰ˆæœ¬è¿›è¡Œä»‹ç»ã€‚<br>\n<a name=\"xfdTU\"></a></p>\n<h4 id=\"Quick-Start\">Quick Start</h4>\n<p><strong>å®‰è£…</strong></p>\n<p><code>pip install git+https://github.com/microsoft/LoRA</code></p>\n<p><strong>ä½¿ç”¨</strong></p>\n<ul>\n<li>åˆ›å»º</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ===== Before =====</span></span><br><span class=\"line\">layer = nn.Linear(in_features, out_features)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ===== After ======</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> loralib <span class=\"keyword\">as</span> lora</span><br><span class=\"line\"><span class=\"comment\"># Add a pair of low-rank adaptation matrices with rank r=16</span></span><br><span class=\"line\">layer = lora.Linear(in_features, out_features, r=<span class=\"number\">16</span>)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>å¾ªç¯</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> loralib <span class=\"keyword\">as</span> lora</span><br><span class=\"line\">model = BigModel()</span><br><span class=\"line\"><span class=\"comment\"># This sets requires_grad to False for all parameters without the string &quot;lora_&quot; in their names</span></span><br><span class=\"line\">lora.mark_only_lora_as_trainable(model)</span><br><span class=\"line\"><span class=\"comment\"># Training loop</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> batch <span class=\"keyword\">in</span> dataloader:</span><br><span class=\"line\">   ...</span><br></pre></td></tr></table></figure>\n<ul>\n<li>ä¿å­˜æ¨¡å‹</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ===== Before =====</span></span><br><span class=\"line\">torch.save(model.state_dict(), checkpoint_path)</span><br><span class=\"line\"><span class=\"comment\"># ===== After =====</span></span><br><span class=\"line\">torch.save(lora.lora_state_dict(model), checkpoint_path)</span><br></pre></td></tr></table></figure>\n<h4 id=\"LoRA-Layer\">LoRA Layer</h4>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LoRALayer</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        self, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        r: <span class=\"built_in\">int</span>, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        lora_alpha: <span class=\"built_in\">int</span>, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        lora_dropout: <span class=\"built_in\">float</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        merge_weights: <span class=\"built_in\">bool</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    </span>):</span></span><br><span class=\"line\">        self.r = r</span><br><span class=\"line\">        self.lora_alpha = lora_alpha</span><br><span class=\"line\">        <span class=\"comment\"># Optional dropout</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> lora_dropout &gt; <span class=\"number\">0.</span>:</span><br><span class=\"line\">            self.lora_dropout = nn.Dropout(p=lora_dropout)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.lora_dropout = <span class=\"keyword\">lambda</span> x: x</span><br><span class=\"line\">        <span class=\"comment\"># Mark the weight as unmerged</span></span><br><span class=\"line\">        self.merged = <span class=\"literal\">False</span></span><br><span class=\"line\">        self.merge_weights = merge_weights</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Linear</span>(<span class=\"params\">nn.Linear, LoRALayer</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># LoRA implemented in a dense layer</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        self, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        in_features: <span class=\"built_in\">int</span>, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        out_features: <span class=\"built_in\">int</span>, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        r: <span class=\"built_in\">int</span> = <span class=\"number\">0</span>, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        lora_alpha: <span class=\"built_in\">int</span> = <span class=\"number\">1</span>, </span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        lora_dropout: <span class=\"built_in\">float</span> = <span class=\"number\">0.</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        fan_in_fan_out: <span class=\"built_in\">bool</span> = <span class=\"literal\">False</span>, <span class=\"comment\"># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span></span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        merge_weights: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">        **kwargs</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">    </span>):</span></span><br><span class=\"line\">        nn.Linear.__init__(self, in_features, out_features, **kwargs)</span><br><span class=\"line\">        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,</span><br><span class=\"line\">                           merge_weights=merge_weights)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.fan_in_fan_out = fan_in_fan_out</span><br><span class=\"line\">        <span class=\"comment\"># Actual trainable parameters</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> r &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))</span><br><span class=\"line\">            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))</span><br><span class=\"line\">            self.scaling = self.lora_alpha / self.r</span><br><span class=\"line\">            <span class=\"comment\"># Freezing the pre-trained weight matrix</span></span><br><span class=\"line\">            self.weight.requires_grad = <span class=\"literal\">False</span></span><br><span class=\"line\">        self.reset_parameters()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> fan_in_fan_out:</span><br><span class=\"line\">            self.weight.data = self.weight.data.T</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reset_parameters</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        nn.Linear.reset_parameters(self)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">hasattr</span>(self, <span class=\"string\">&#x27;lora_A&#x27;</span>):</span><br><span class=\"line\">            <span class=\"comment\"># initialize A the same way as the default for nn.Linear and B to zero</span></span><br><span class=\"line\">            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(<span class=\"number\">5</span>))</span><br><span class=\"line\">            nn.init.zeros_(self.lora_B)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span>(<span class=\"params\">self, mode: <span class=\"built_in\">bool</span> = <span class=\"literal\">True</span></span>):</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">T</span>(<span class=\"params\">w</span>):</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> w.T <span class=\"keyword\">if</span> self.fan_in_fan_out <span class=\"keyword\">else</span> w</span><br><span class=\"line\">        nn.Linear.train(self, mode)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.merge_weights <span class=\"keyword\">and</span> self.merged:</span><br><span class=\"line\">            <span class=\"comment\"># Make sure that the weights are not merged</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.r &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling</span><br><span class=\"line\">            self.merged = <span class=\"literal\">False</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">eval</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">T</span>(<span class=\"params\">w</span>):</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> w.T <span class=\"keyword\">if</span> self.fan_in_fan_out <span class=\"keyword\">else</span> w</span><br><span class=\"line\">        nn.Linear.<span class=\"built_in\">eval</span>(self)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.merge_weights <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> self.merged:</span><br><span class=\"line\">            <span class=\"comment\"># Merge the weights and mark it</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.r &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling</span><br><span class=\"line\">            self.merged = <span class=\"literal\">True</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x: torch.Tensor</span>):</span></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">T</span>(<span class=\"params\">w</span>):</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> w.T <span class=\"keyword\">if</span> self.fan_in_fan_out <span class=\"keyword\">else</span> w</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.r &gt; <span class=\"number\">0</span> <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> self.merged:</span><br><span class=\"line\">            result = F.linear(x, T(self.weight), bias=self.bias)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.r &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                result += (self.lora_dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling</span><br><span class=\"line\">            <span class=\"keyword\">return</span> result</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> F.linear(x, T(self.weight), bias=self.bias)</span><br></pre></td></tr></table></figure>\n","tags":["Efficient Framework","Large Language Model"]},{"title":"DDPM as Example of Variational Inference","url":"/posts/DDPM_as_Example_of_Variational_Inference/","content":"<blockquote>\n<p>å¾ˆå¤šæ¬¡ç¿»çœ‹DDPMï¼Œå§‹ç»ˆä¸å¤ªèƒ½ç†è§£è®ºæ–‡ä¸­æåˆ°çš„$\\text{Variational Inference}$åˆ°åº•æ˜¯å¦‚ä½•åœ¨è¿™ä¸ªå·¥ä½œä¸­èµ·åˆ°ä½œç”¨ã€‚äº”ä¸€å‡æœŸåœ¨å®¶ï¼Œæ— æ„é—´åˆåˆ·åˆ°å¾äº¦è¾¾è€å¸ˆæ—©äº›å¹´å½•åˆ¶çš„ç†è®ºè§†é¢‘ï¼Œæ²¡æƒ³åˆ°å…¶ä¸­ä¹Ÿæœ‰ä»‹ç»è¿™éƒ¨åˆ†çš„å†…å®¹ã€‚è€å¸ˆçš„ä¸Šè¯¾æ–¹å¼æ€»æ˜¯å¨“å¨“é“æ¥ï¼ŒæŠŠæ¯ä¸€æ­¥éƒ½è®²è§£å¾—å¾ˆä»”ç»†ã€‚æœ¬æ–‡è®°å½•ä¸€ä¸‹ä¸ªäººå¯¹å¼€å¤´é—®é¢˜çš„æ€è€ƒã€‚</p>\n</blockquote>\n<a id=\"more\"></a>\n<h1>Background</h1>\n<p>å¦‚æœéœ€è¦ç®€ç•¥åœ°ä»‹ç»ä¸€ä¸‹<a href=\"https://arxiv.org/abs/2006.11239\">DDPM</a>è¿™ä¸ªå·¥ä½œï¼Œå¯èƒ½ä¼šç”¨ä»¥ä¸‹å‡ å¥è¯ç®€å•åœ°æè¿°ï¼š<code>DDPM</code>ä»¥<code>Markov</code>çš„å½¢å¼å¯¹æ•°æ®ï¼ˆå›¾ç‰‡ï¼‰â€œæ‰©æ•£è¿‡ç¨‹â€å»ºæ¨¡ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒæ‹Ÿåˆï¼Œå­¦ä¹ æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>\n<p>æ‰€ä»¥å¯¹äºç”Ÿæˆä»»åŠ¡æ¥è¯´ï¼Œå¸Œæœ›ä»ç»™å®šæ•°æ®ä¸­å­¦ä¹ åˆ°çš„æ˜¯æ•°æ®çš„æ½œåœ¨ä¿¡æ¯ã€‚æ¯”å¦‚å›¾ç‰‡ç”Ÿæˆï¼Œåœ¨ç»™å®šä¸€äº›å›¾ç‰‡åï¼Œæ¨¡å‹å­¦ä¹ åˆ°çš„æ˜¯â€œæ­£å¸¸å›¾ç‰‡é•¿ä»€ä¹ˆæ ·å­â€ï¼Œå¦‚ï¼š</p>\n<ol>\n<li>ä¸€å¼ åŒ…å«æ‰‹æœºæ­£é¢çš„å›¾ç‰‡ä¼šæœ‰ã€æ‰‹æœºå±å¹•ã€‘ï¼›</li>\n<li>ä¸€å¼ åŒ…å«çŒ«å’ªçš„å›¾ç‰‡ä¼šæœ‰äººä»¬è§‚å¯Ÿåˆ°çš„çŒ«å’ªæ¨¡æ ·ï¼›</li>\n<li>â€¦</li>\n</ol>\n<p>å¯¹äºå›¾ç‰‡ä¸­æ¯ä¸ªåƒç´ ç‚¹å’Œé™„è¿‘çš„åƒç´ ç‚¹ï¼Œè¿›è¡Œâ€œåˆç†â€å¸ƒå±€ï¼Œæ‰èƒ½ç”Ÿæˆâ€œç¬¦åˆäººä»¬è®¤çŸ¥çš„å›¾ç‰‡â€ã€‚</p>\n<p><strong>å›¾ç‰‡ç”Ÿæˆ</strong>èƒ½åƒå¸¸è§çš„æœºå™¨å­¦ä¹ ä»»åŠ¡å¦‚åˆ†ç±»ä»»åŠ¡ã€å›å½’ä»»åŠ¡ï¼Œèƒ½åŸºäº<code>maximize likelihood</code>çš„å½¢å¼æ¥è®­ç»ƒä¹ˆï¼Ÿ</p>\n<p>**ç»“è®ºæ˜¯å¾ˆéš¾ï¼Œ**å…ˆå›é¡¾å¦‚ä½•åš<code>maximum likelihood</code>ã€‚ç»™å®šä¸€æ‰¹æ•°æ®ï¼Œé¦–å…ˆéœ€è¦å‡å®šæ•°æ®æœä»çš„åˆ†å¸ƒï¼Œæ¥ç€å†™å‡ºä¼¼ç„¶å‡½æ•°ï¼Œä¹‹åç›´æ¥é€šè¿‡è§£æè§£çš„å½¢å¼æˆ–æ˜¯æ¢¯åº¦ä¸‹é™çš„å½¢å¼ï¼Œæ±‚å‡ºåˆ†å¸ƒã€‚</p>\n<p>é—®é¢˜å°±å‡ºåœ¨<strong>å‡å®šåˆ†å¸ƒ</strong>è¿™ä¸€æ­¥ï¼Œæ²¡æœ‰äººçŸ¥é“å›¾ç‰‡å®¢è§‚ä¸Šæœä»ä»€ä¹ˆåˆ†å¸ƒã€‚é‚£å¦‚æœä½¿ç”¨ç¥ç»ç½‘ç»œç›´æ¥æ‹Ÿåˆå¯ä»¥ä¹ˆï¼Ÿè¿™å¥½åƒä¹Ÿä¸ç°å®ï¼Œæ‹¿ä¸€å¼ <code>512*512*3</code>çš„å›¾ç‰‡æ¥è¯´ï¼Œç½‘ç»œè¾“å‡ºå±‚å…±æœ‰çº¦<code>75w</code>çš„æ•°å€¼ã€‚</p>\n<p>å¯¹äºå›¾ç‰‡ç”Ÿæˆè¿˜æœ‰å¦å¤–ä¸€ä¸ªé—®é¢˜ï¼Œä¸–ç•Œä¸Šçš„<strong>å›¾ç‰‡å¤ªå¤šäº†</strong>ï¼Œç›®ä¹‹æ‰€åŠç¨åšå¤„ç†ï¼Œçš†ä¸ºå›¾ç‰‡ã€‚å³ä¾¿ä½¿ç”¨ç¥ç»ç½‘ç»œèƒ½æ‹Ÿåˆï¼Œæœ€åç”Ÿæˆçš„å›¾ç‰‡å¾ˆéš¾å­˜åœ¨å¤šæ ·æ€§ã€‚</p>\n<p>é‚£ç›®å‰å›¾ç‰‡ç”Ÿæˆæ¨¡å‹éƒ½æ˜¯æ€ä¹ˆåšçš„ï¼Œæ¯”å¦‚<code>VAE</code>æˆ–æ˜¯æœ¬æ–‡å³å°†è¦ä»‹ç»çš„<code>Diffusion Model</code>ï¼Œå®ƒä»¬å­¦ä¹ çš„éƒ½æ˜¯æ•°æ®åˆ†å¸ƒ$p(x)$ï¼Œä½†ç›´æ¥æ±‚$p(x)$è¿™ä¹ˆéº»çƒ¦ï¼Œéœ€è¦æ€ä¹ˆåšï¼Ÿè¿™å…¶å®ä¹Ÿæ˜¯$\\text{Variational Inference}$çš„æ ¸å¿ƒæ€æƒ³ï¼Œâ€œæ›²çº¿æ•‘å›½â€ï¼Œé€šè¿‡å¼•å…¥å…¶å®ƒåˆ†å¸ƒï¼Œå°†åŸæœ¬éš¾ä»¥ä¼˜åŒ–çš„é—®é¢˜è½¬å˜ä¸ºå¯ä¼˜åŒ–é—®é¢˜ã€‚</p>\n<h1>ELOB</h1>\n<p>å…ˆæŠŠä¸Šè¿°æåˆ°çš„æ‰€æœ‰èƒŒæ™¯å…ˆæŠ›å¼€ï¼Œç ”ç©¶ä¸€ä¸‹$p(x)$ï¼Œçœ‹çœ‹èƒ½å¾—åˆ°ä»€ä¹ˆæœ‰æ„æ€çš„ç»“è®ºã€‚</p>\n<p>a. åŸºäºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼Œå¼•å…¥æ–°çš„éšæœºå˜é‡$z$ï¼š$p(x) = \\frac{p(x, z)}{p(z\\mid x)}$ï¼›</p>\n<p>b. å¯¹äºä¸¤è¾¹åŒæ—¶å–$\\ln$ï¼Œç­‰å¼ä¾ç„¶æˆç«‹ï¼Œå› æ­¤æœ‰ï¼š$\\ln{p(x)} = \\ln{\\frac{p(x, z)}{p(z \\mid x)}}$ï¼›</p>\n<p>c. å³è¾¹åˆ†å­åˆ†æ¯åŒä¹˜ä»¥$q(z)$ï¼š$\\ln{p(x)} = \\ln{\\frac{p(x, z) * q(z)}{p(z \\mid x) * q(z)}} = \\ln{\\left(\\frac{p(x, z)}{q(z)} * \\frac{q(z)}{p(z \\mid x)}\\right)} = \\ln{\\frac{p(x, z)}{q(z)}} + \\ln{\\frac{q(z)}{p(z \\mid x)}}$</p>\n<p>d. å†æ¬¡ï¼Œå¯¹äºä¸Šå¼å·¦å³ä¸¤è¾¹æ±‚å…³äº$q(z)$çš„æœŸæœ›ï¼Œç­‰å¼ä¾ç„¶æˆç«‹ï¼š</p>\n<p>$$<br>\n\\begin{aligned}<br>\n&amp;\\mathbb{E}<em>{z\\sim q(z)}{[\\ln{p(x)}]} = \\mathbb{E}</em>{z\\sim q(z)}{(\\ln{\\frac{p(x, z)}{q(z)}} + \\ln{\\frac{q(z)}{p(z \\mid x)}})} \\<br>\n\\iff &amp; \\int_z q(z)\\ln{p(x)}dz = \\int_z q(z)\\ln{\\frac{p(x, z)}{q(z)}}dz + \\int_z q(z)\\ln{\\frac{q(z)}{p(z \\mid x)}}dz \\<br>\n\\iff &amp; \\ln{p(x)} = \\int_z q(z)\\ln{\\frac{p(x, z)}{q(z)}}dz + \\int_z q(z)\\ln{\\frac{q(z)}{p(z \\mid x)}}dz<br>\n\\end{aligned}<br>\n\\tag{1}<br>\n$$</p>\n<p>ä¸€ç³»åˆ—å˜æ¢åï¼Œ$(1)$å¼æ˜¯æœ€åçš„æ¨å¯¼ç»“æœï¼Œç­‰å¼å³è¾¹ç”±ä¸¤ä¸ªé¡¹ç»„æˆã€‚ç¬¬äºŒä¸ªé¡¹$\\int_z q(z)\\ln{\\frac{q(z)}{p(z \\mid x)}}dz$ï¼Œå«åš<a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">KLæ•£åº¦</a>ï¼Œå®ƒè¢«ç”¨æ¥è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„â€œè·ç¦»â€ï¼Œ<strong>æ€§è´¨æ˜¯å€¼ä¸å°äº0</strong>ã€‚</p>\n<p>è¿™æ ·ä¸€æ¥ï¼Œé€šè¿‡$(1)$å¯ä»¥å¾—åˆ°ä¸ç­‰å¼$(2)$ï¼š</p>\n<p>$$<br>\n\\begin{equation*}<br>\n\\ln{p(x)} \\geq \\int_z q(z)\\ln{\\frac{p(x, z)}{q(z)}}dz<br>\n\\end{equation*}<br>\n\\tag{2}<br>\n$$</p>\n<p>$(1)$å¼å³è¾¹çš„ç¬¬ä¸€é¡¹ï¼ŒåŒæ—¶ä¹Ÿæ˜¯$(2)$å¼çš„å³è¾¹é¡¹ï¼Œè¢«å­¦è€…ä»¬å«åš$\\text{ELBO(Evidence Lower Bound)}$ã€‚</p>\n<h1>Objective Function</h1>\n<p>ä¸Šè¿°æ¨å¯¼çš„$(2)$å¼å¯ä»¥è¢«è§†ä½œâ€œå®šç†â€ä¸€èˆ¬çš„å­˜åœ¨ï¼Œå³å¯¹äºæŸä¸ªåˆ†å¸ƒçš„å¯¹æ•°å½¢å¼ï¼Œæ€»å¯ä»¥æ‰¾åˆ°å®ƒçš„ä¸‹ç•Œã€‚</p>\n<p>é‚£$(2)$å¼å¯ä»¥ç”¨æ¥åšä»€ä¹ˆï¼Ÿåœ¨<code>Background</code>ä¸­æåˆ°ï¼Œå›¾ç‰‡ç”Ÿæˆä»»åŠ¡ä¸­çš„$p(x)$æƒ³è¦å¯¹å®ƒåš<code>maximum likelihood</code>æ ¹æœ¬æ— æ³•åšèµ·ã€‚ç›®æ ‡ä¾ç„¶æ˜¯æœ€å¤§åŒ–$p(x)$ï¼Œä½†æœ‰äº†$(2)$å¼ï¼Œæ±‚è§£çš„ç›®æ ‡å¯ä»¥è½¬ç§»åˆ°æœ€å¤§åŒ–å®ƒçš„ä¸‹ç•Œ$\\text{ELBO}$ã€‚</p>\n<p>è¿™ä¹Ÿæ˜¯è®ºæ–‡ä¸­æåˆ°çš„ï¼š</p>\n<blockquote>\n<p>This paper presents progress in diffusion probabilistic models. A diffusion probabilistic model (which we will call a â€œdiffusion modelâ€ for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time.</p>\n</blockquote>\n<p>æ¥ä¸‹æ¥ï¼Œå›åˆ°è®ºæ–‡ä¸­ï¼Œçœ‹çœ‹æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥æ¨å¯¼å‡º<code>DDPM</code>çš„ä¼˜åŒ–ç›®æ ‡ã€‚$(3)$å¼ç›´æ¥æ‘˜å½•äºè®ºæ–‡ï¼š</p>\n<p>$$<br>\n\\begin{equation*}<br>\n\\ln{p(x)} \\geq \\int_z q(z)\\ln{\\frac{p(x, z)}{q(z)}}dz = \\mathbb{E}_{z \\sim q(z)}\\left[\\ln{\\frac{p(x,z)}{q(z)}}\\right]<br>\n\\end{equation*}<br>\n\\tag{2}<br>\n$$</p>\n<p>$$<br>\n\\begin{equation*}<br>\n\\mathbb{E}\\left[-\\log p_\\theta\\left(\\mathbf{x}<em>0\\right)\\right] \\leq \\mathbb{E}<em>q\\left[-\\log \\frac{p</em>\\theta\\left(\\mathbf{x}</em>{0: T}\\right)}{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}<em>0\\right)}\\right]=\\mathbb{E}<em>q\\left[-\\log p\\left(\\mathbf{x}<em>T\\right)-\\sum</em>{t \\geq 1} \\log \\frac{p</em>\\theta\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}_t\\right)}{q\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right)}\\right]=: L<br>\n\\end{equation*}<br>\n\\tag{3}<br>\n$$</p>\n<p>ä¸‹é¢ä¸€é¡¹é¡¹åœ°å¯¹$(3)$ è¿›è¡Œæ‹†è§£ï¼Œå¹¶ä¸”å°†å®ƒä¸$(2)$æ¯”å¯¹ï¼Œèƒ½å¸®åŠ©æ›´å¥½åœ°ç†è§£ï¼š</p>\n<ol>\n<li>\n<p>$(3)$ä¸ç­‰å·å·¦è¾¹çš„$\\mathbb{E}\\left[-\\log p_\\theta\\left(\\mathbf{x}<em>0\\right)\\right]$è¿›ä¸€æ­¥åŒ–ç®€å°±æ˜¯$-\\log p</em>\\theta\\left(\\mathbf{x}<em>0\\right)$ã€‚å…¶ä¸­ï¼Œ$p</em>\\theta\\left(\\mathbf{x}_0\\right)$ä¾¿æ˜¯æ¨¡å‹è¦å­¦ä¹ çš„æœ€ç»ˆç›®æ ‡ï¼šå›¾åƒçš„åˆ†å¸ƒï¼Œ$\\theta$æ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œ$\\mathbf{x}_0$æ˜¯å›¾ç‰‡ï¼›</p>\n</li>\n<li>\n<p>$(2)$å¼çš„å·¦å³ä¸¤è¾¹åŒæ—¶åŠ ä¸Šç¬¦å·ï¼Œ$\\geq$å˜ä¸º$\\leq$ï¼›</p>\n</li>\n<li>\n<p>çœ‹$(3)$ä¸ç­‰å¼å³è¾¹éƒ¨åˆ†ï¼Œ$\\mathbb{E}<em>q\\left[-\\log \\frac{p</em>\\theta\\left(\\mathbf{x}<em>{0: T}\\right)}{q\\left(\\mathbf{x}</em>{1: T} \\mid \\mathbf{x}_0\\right)}\\right]$</p>\n<ol>\n<li>\n<p>å¾ˆæ˜æ˜¾ï¼Œ$q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)$ç›¸å½“äº$(2)$ä¸­å¼•å…¥çš„é¢å¤–åˆ†å¸ƒ$q(z)$ã€‚å¯¹äº$z$ï¼Œåœ¨ç”Ÿæˆæ¨¡å‹ä¸­ä¼šç»™å®ƒä¸€ä¸ªç§°å‘¼ï¼šéšå˜é‡$(\\text{latent})$ã€‚å®é™…ä¸Šï¼Œåœ¨<code>diffusion models</code>é‡Œï¼Œå¯¹$\\mathbf{x}_0$åŠ å™ªåçš„$\\mathbf{x}_1,\\mathbf{x}_2,\\ldots, \\mathbf{x}_T$å°±å¯ä»¥çœ‹ä½œéšå˜é‡ï¼Œé‚£ä¸å¦¨è®°ä½œ$z := {\\mathbf{x}_1,\\mathbf{x}_2,\\ldots, \\mathbf{x}_T}$ï¼›</p>\n</li>\n<li>\n<p>$p_\\theta\\left(\\mathbf{x}<em>{0: T}\\right) = p</em>\\theta\\left(\\mathbf{x}<em>{0}, \\mathbf{x}</em>{1}, \\ldots, \\mathbf{x}_{T}\\right)$ï¼Œæ˜¯å…³äº$\\mathbf{x}_0, z$çš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œå› ä¸ºé€‰ç”¨é©¬å°”ä»£å¤«é“¾å»ºæ¨¡ï¼Œé‚£ä¹ˆä¾æ®é©¬å°”å¯å¤«é“¾çš„æ€§è´¨ï¼Œè®ºæ–‡å®šä¹‰ï¼š</p>\n</li>\n</ol>\n</li>\n</ol>\n<p>$$<br>\n\\begin{equation*}<br>\n\\begin{aligned}<br>\nq\\left(\\mathbf{x}<em>{1: T} \\mid \\mathbf{x}<em>0\\right)&amp;:=\\prod</em>{t=1}^T q\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right) \\<br>\np</em>\\theta\\left(\\mathbf{x}<em>{0: T}\\right)&amp;:=p\\left(\\mathbf{x}<em>T\\right) \\prod</em>{t=1}^T p</em>\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right)<br>\n\\end{aligned}<br>\n\\end{equation*}<br>\n\\tag{4}<br>\n$$</p>\n<ol start=\"3\">\n<li>å°†$(4)$å¸¦å…¥$(3)$ä¸ç­‰å¼å³è¾¹çš„ç¬¬ä¸€é¡¹ï¼Œå¾—åˆ°$L$ï¼š</li>\n</ol>\n<p>$$<br>\n\\begin{equation*}<br>\n\\begin{aligned}<br>\n&amp;\\mathbb{E}<em>q\\left[-\\log \\frac{p</em>\\theta\\left(\\mathbf{x}<em>{0: T}\\right)}{q\\left(\\mathbf{x}</em>{1: T} \\mid \\mathbf{x}<em>0\\right)}\\right] \\<br>\n=&amp;\\mathbb{E}<em>q\\left[-\\log \\frac{p\\left(\\mathbf{x}<em>T\\right) \\prod</em>{t=1}^T p</em>\\theta\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t\\right)}{\\prod</em>{t=1}^T q\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right)}\\right] \\<br>\n=&amp;\\mathbb{E}<em>q\\left[-\\log p\\left(\\mathbf{x}<em>T\\right)-\\sum</em>{t \\geq 1} \\log \\frac{p</em>\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t\\right)}{q\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right)}\\right] := L<br>\n\\end{aligned}<br>\n\\end{equation*}<br>\n$$</p>\n<p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œç»è¿‡äº†å¾ˆå¤šè½®çš„å˜æ¢ä»¥åŠæ•°å­¦å…¬å¼ï¼Œå…ˆæ‹ä¸€éï¼Œå†å¾€ä¸‹ã€‚$L$æ˜¯ä¸€ä¸ªæ›¿ä»£çš„ä¼˜åŒ–ç›®æ ‡ï¼Œ<br>\n$$\\mathop{\\arg\\min}{(L)} \\iff \\mathop{\\arg\\min}{(-\\ln{p}_{\\theta}(\\mathbf{x}<em>0))} \\iff \\mathop{\\arg\\max}{(\\ln{p}</em>{\\theta}(\\mathbf{x}_0))}$$</p>\n<p>æ¥ä¸‹æ¥ï¼Œè®ºæ–‡ä¸­å¯¹$L$è¿›è¡Œäº†é‡å†™ï¼Œä»¥ä¸‹æ­¥éª¤ç›´æ¥æ‘˜å½•è‡ªè®ºæ–‡$\\text{Appendix A}$</p>\n<p>$$<br>\n\\begin{equation*}<br>\n\\begin{aligned}<br>\nL &amp; =\\mathbb{E}<em>q\\left[-\\log \\frac{p</em>\\theta\\left(\\mathbf{x}<em>{0: T}\\right)}{q\\left(\\mathbf{x}</em>{1: T} \\mid \\mathbf{x}<em>0\\right)}\\right] \\ &amp; =\\mathbb{E}<em>q\\left[-\\log p\\left(\\mathbf{x}<em>T\\right)-\\sum</em>{t \\geq 1} \\log \\frac{p</em>\\theta\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t\\right)}{q\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right)}\\right] \\ &amp; =\\mathbb{E}<em>q\\left[-\\log p\\left(\\mathbf{x}<em>T\\right)-\\sum</em>{t&gt;1} \\log \\frac{p</em>\\theta\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t\\right)}{q\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right)}-\\log \\frac{p</em>\\theta\\left(\\mathbf{x}_0 \\mid \\mathbf{x}_1\\right)}{q\\left(\\mathbf{x}_1 \\mid \\mathbf{x}<em>0\\right)}\\right] \\<br>\n&amp;=\\mathbb{E}<em>q\\left[-\\log p\\left(\\mathbf{x}<em>T\\right)-\\sum</em>{t&gt;1} \\log \\left[\\frac{p</em>\\theta\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t\\right)}{q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}<em>0\\right)} \\cdot \\frac{q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}_0\\right)}{q\\left(\\mathbf{x}_t \\mid \\mathbf{x}<em>0\\right)}\\right]-\\log \\frac{p</em>\\theta\\left(\\mathbf{x}_0 \\mid \\mathbf{x}_1\\right)}{q\\left(\\mathbf{x}_1 \\mid \\mathbf{x}_0\\right)}\\right]<br>\n\\end{aligned}<br>\n\\end{equation*}<br>\n\\tag{5}<br>\n$$</p>\n<p>å€’æ•°ä¸¤æ­¥çš„å˜æ¢å‘ç”Ÿåœ¨ç¬¬äºŒé¡¹ï¼Œå…·ä½“ä¾æ®ä¸ºï¼š</p>\n<p>$$<br>\n\\begin{aligned}<br>\nq\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right)<br>\n=&amp; \\frac{q\\left(\\mathbf{x}<em>t, \\mathbf{x}</em>{t-1}\\right)}{q\\left(\\mathbf{x}<em>{t-1}\\right)} \\<br>\n=&amp; \\frac{q\\left(\\mathbf{x}<em>t, \\mathbf{x}</em>{t-1} \\mid \\mathbf{x}</em>{0}\\right) *q(\\mathbf{x}<em>{0})}{q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>{0}\\right) * q(\\mathbf{x}</em>{0})} \\<br>\n=&amp; \\frac{q\\left(\\mathbf{x}<em>t, \\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>{0}\\right) }{q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>0\\right)}<br>\n\\end{aligned}<br>\n\\quad \\Rightarrow \\quad<br>\n\\begin{aligned}<br>\n&amp;\\sum</em>{t&gt;1} \\log \\frac{p_\\theta\\left(\\mathbf{x}<em>{t-1} \\mid \\mathbf{x}<em>t\\right)}{q\\left(\\mathbf{x}<em>t \\mid \\mathbf{x}</em>{t-1}\\right)} \\<br>\n=&amp; \\sum</em>{t&gt;1} \\log \\frac{p</em>\\theta\\left(\\mathbf{x}<em>{t-1} \\mid \\mathbf{x}<em>t\\right)}{q\\left(\\mathbf{x}<em>t, \\mathbf{x}</em>{t-1} \\mid \\mathbf{x}</em>{0}\\right) } \\cdot {q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>0\\right)} \\<br>\n=&amp; \\sum</em>{t&gt;1} \\log \\frac{p_\\theta\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}<em>t\\right)}{q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}<em>0\\right)} \\cdot \\frac{q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}_0\\right)}{q\\left(\\mathbf{x}_t \\mid \\mathbf{x}_0\\right)}<br>\n\\end{aligned}<br>\n$$</p>\n<p>æ¥ç€å¯¹$(5)$è¿›è¡Œæ”¹å†™å¾—åˆ°æœ€ç»ˆå½¢å¼$(6)$ï¼š<br>\n$$<br>\n\\begin{aligned}<br>\nL &amp;=\\mathbb{E}_q\\left[-\\log \\frac{p\\left(\\mathbf{x}<em>T\\right)}{q\\left(\\mathbf{x}<em>T \\mid \\mathbf{x}<em>0\\right)}-\\sum</em>{t&gt;1} \\log \\frac{p</em>\\theta\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t\\right)}{q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t, \\mathbf{x}<em>0\\right)}-\\log p</em>\\theta\\left(\\mathbf{x}<em>0 \\mid \\mathbf{x}<em>1\\right)\\right] \\<br>\n&amp;=\\mathbb{E}<em>q[\\underbrace{D</em>{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}<em>T \\mid \\mathbf{x}<em>0\\right) | p\\left(\\mathbf{x}<em>T\\right)\\right)}</em>{L_T}+\\sum</em>{t&gt;1} \\underbrace{D</em>{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t, \\mathbf{x}<em>0\\right) | p</em>\\theta\\left(\\mathbf{x}</em>{t-1} \\mid \\mathbf{x}<em>t\\right)\\right)}</em>{L</em>{t-1}} \\underbrace{-\\log p</em>\\theta\\left(\\mathbf{x}_0 \\mid \\mathbf{x}<em>1\\right)}</em>{L_0}]<br>\n\\end{aligned}<br>\n\\tag{6}<br>\n$$</p>\n<h1>Summary</h1>\n<p>å¤ªå¥½äº†ï¼Œå¯¹äº$(6)$æ¥è¯´ï¼Œå®ƒæœ€èµ·ç æ˜¯ä¸ªå¯ä»¥ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°äº†ï¼Œå› ä¸ºè®ºæ–‡ä¸­å®šä¹‰é©¬å°”å¯å¤«é“¾ç›¸é‚»çŠ¶æ€çš„è½¬å˜æ˜¯æœä»é«˜æ–¯åˆ†å¸ƒçš„ã€‚å½“ç„¶åœ¨è®ºæ–‡ä¸­ï¼Œ$(6)$è¿˜ä¼šè¿›ä¸€æ­¥è¢«æ”¹å†™ï¼Œå¾—åˆ°æ›´åŠ ç²¾ç®€çš„$\\text{loss function}$å½¢å¼ã€‚<br>\n<code>DDPM</code>æ˜¯åº”ç”¨$\\text{variational inference}$è¿›è¡Œä¼˜åŒ–æ±‚è§£çš„å…¸å‹ä¾‹å­ï¼Œå¾ˆå€¼å¾—å€Ÿé‰´å­¦ä¹ ã€‚</p>\n<h1>Reference</h1>\n<ul>\n<li><a href=\"https://www.youtube.com/playlist?list=PLyAft-JyjIYoN_6X932U_-ZlHKdInFrUV\">Variational Inference Basic, Xu, R.Y.D</a></li>\n<li><a href=\"https://arxiv.org/abs/2006.11239\">Denoising Diffusion Probabilistic Models</a></li>\n</ul>\n","tags":["Diffusion Model","Math Heavy"]}]